Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Hastie2009,
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Hastie, Trevor, Tibshirani, Robert, Friedman}, Jerome},
booktitle = {The Elements of Statistical Learning Data Mining, Inference, and Prediction},
doi = {10.1007/978-0-387-84858-7},
edition = {Second},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-0-387-84858-7},
issn = {00111287},
keywords = {Data Mining,Inference,Neural Nets,Prediction,Statistical Learning},
pages = {282},
pmid = {12377617},
publisher = {Springer Series in Statistics},
title = {{The Elements of Statistical Learning Data Mining, Inference, and Prediction}},
url = {http://www.worldcat.org/oclc/405547558 Hastie, Tibshirani et al - The elements of statistical learning.pdf http://www.springer.com.libproxy1.nus.edu.sg/statistics/statistical+theory+and+methods/book/978-0-387-84857-0 http://statweb.stanford.edu/{~}tibs/E ht},
year = {2009}
}

@article{Cade2015,
author = {Cade, Brian S.},
doi = {10.1890/14-1639.1},
issn = {00129658},
journal = {Ecology},
keywords = {Generalized linear models,Greater Sage-Grouse,Model averaging,Multicollinearity,Multimodel inference,Partial effects,Partial standard deviations,Regression coefficients,Relative importance of predictors,Species distribution models,Zero-truncated Poisson regression},
month = {sep},
number = {9},
pages = {2370--2382},
publisher = {Ecological Society of America},
title = {{Model averaging and muddled multimodel inferences}},
volume = {96},
year = {2015}
}


@article{Piironen2020,
archivePrefix = {arXiv},
arxivId = {1810.02406},
author = {Piironen, Juho and Paasiniemi, Markus and Vehtari, Aki},
doi = {10.1214/20-EJS1711},
eprint = {1810.02406},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Feature selection,Post-selection inference,Prediction,Projection,Sparsity},
number = {1},
pages = {2155--2197},
title = {{Projective inference in high-dimensional problems: Prediction and feature selection}},
url = {https://doi.org/10.1214/20-EJS1711},
volume = {14},
year = {2020}
}

@article{Link2006,
author = {Link, William A. and Barker, Richard J.},
doi = {10.1890/0012-9658(2006)87[2626:MWATFO]2.0.CO;2},
issn = {00129658},
journal = {Ecology},
keywords = {AIC,Akaike's information criterion,BIC,Bayes factors,Bayesian inference,Bayesian information criterion,Model averaging,Model selection,Salmo trutta},
month = {oct},
number = {10},
pages = {2626--2635},
pmid = {17089670},
title = {{Model weights and the foundations of multimodel inference}},
volume = {87},
year = {2006}
}




@article{Gabry2019,
author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
title = {Visualization in Bayesian workflow},
journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
volume = {182},
number = {2},
pages = {389-402},
keywords = {Bayesian data analysis, Statistical graphics, Statistical workflow},
doi = {10.1111/rssa.12378},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssa.12378},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssa.12378},
year = {2019}
}


@phdthesis{Wilson2004,
title={Habitat quality, competition, and recruitment processes in two marine gobies}, author={Wilson, J A}, year={2004}
}


@article{Valavi2019,
author = {Valavi, Roozbeh and Elith, Jane and Lahoz-Monfort, José J. and Guillera-Arroita, Gurutzeta},
title = {blockCV: An $R$ package for generating spatially or environmentally separated folds for $k$-fold cross-validation of species distribution models},
journal = {Methods in Ecology and Evolution},
volume = {10},
number = {2},
pages = {225-232},
keywords = {block cross-validation, environmental blocking, model evaluation, spatial autocorrelation, spatial blocking, spatial leave-one-out, species distribution modelling, structured environment},
doi = {10.1111/2041-210X.13107},
url = {https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13107},
eprint = {https://besjournals.onlinelibrary.wiley.com/doi/pdf/10.1111/2041-210X.13107},
year = {2019}
}


@article{Dormann2018,
abstract = {In ecology, the true causal structure for a given problem is oſten not known, and several plausible models and thus model predictions exist. It has been claimed that using weighted averages of these models can reduce prediction error, as well as better reflect model selection uncertainty. These claims, however, are oſten demonstrated by isolated examples. Analysts must better understand under which conditions model averaging can improve predictions and their uncertainty estimates. Moreover, a large range of different model averaging methods exists, raising the question of how they differ regarding in their behaviour and performance. Here, we review the mathematical foundations of model averaging along with the diversity of approaches available.We explain that the error in model-averaged predictions depends on each model's predictive bias and variance, as well as the covariance in predictions between models and uncertainty about model weights. We show that model averaging is particularly useful if the predictive error of contributing model predictions is dominated by variance, and if the covariance between models is low. For noisy data, which predominate in ecology, these conditions will oſten be met. Many different methods to derive averaging weights exist, from from Bayesian over information-theoretical to cross-validation optimised and resampling approaches. A general recommendation is difficult, because the performance of methods is oſten context-dependent. Importantly, estimating weights creates some additional uncertainty. As a result, estimated model weights may not always outperform arbitrary fixed weights, such as equal weights for all models. When averaging a set of models with many inadequate models, however, estimating model weights will typically be superior to equal weights. We also investigate the quality of the confidence intervals calculated for model-averaged predictions, showing that they differ greatly in behaviour and seldom manage to achieve nominal coverage. Our overall recommendations stress the importance of non-parametric methods such as cross-validation for a reliable uncertainty quantification of model-averaged predictions.},
author = {Dormann, Carsten F. and Calabrese, Justin M. and Guillera-Arroita, Gurutzeta and Matechou, Eleni and Bahn, Volker and Barto{\'{n}}, Kamil and Beale, Colin M. and Ciuti, Simone and Elith, Jane and Gerstner, Katharina and Guelat, J{\'{e}}r{\^{o}}me and Keil, Petr and Lahoz-Monfort, Jos{\'{e}} J. and Pollock, Laura J. and Reineking, Bj{\"{o}}rn and Roberts, David R. and Schr{\"{o}}der, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Wood, Simon N. and W{\"{u}}est, Rafael O. and Hartig, Florian},
doi = {10.1002/ecm.1309},
file = {::;:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/Dormann{\_}et{\_}al-2018-Ecological{\_}Monographs.pdf:pdf},
issn = {15577015},
journal = {Ecological Monographs},
keywords = {AIC weights,ensemble,model averaging,model combination,nominal coverage,prediction averaging,uncertainty},
number = {4},
pages = {485--504},
title = {{Model averaging in ecology: a review of Bayesian, information-theoretic, and tactical approaches for predictive inference}},
volume = {88},
year = {2018}
}
@misc{Johnson2004,
abstract = {Recently, researchers in several areas of ecology and evolution have begun to change the way in which they analyze data and make biological inferences. Rather than the traditional null hypothesis testing approach, they have adopted an approach called model selection, in which several competing hypotheses are simultaneously confronted with data. Model selection can be used to identify a single best model, thus lending support to one particular hypothesis, or it can be used to make inferences based on weighted support from a complete set of competing models. Model selection is widely accepted and well developed in certain fields, most notably in molecular systematics and mark-recapture analysis. However, it is now gaining support in several other areas, from molecular evolution to landscape ecology. Here, we outline the steps of model selection and highlight several ways that it is now being implemented. By adopting this approach, researchers in ecology and evolution will find a valuable alternative to traditional null hypothesis testing, especially when more than one hypothesis is plausible.},
author = {Johnson, Jerald B. and Omland, Kristian S.},
booktitle = {Trends in Ecology and Evolution},
doi = {10.1016/j.tree.2003.10.013},
file = {::},
issn = {01695347},
number = {2},
pages = {101--108},
publisher = {Elsevier Ltd},
title = {{Model selection in ecology and evolution}},
volume = {19},
year = {2004}
}
@article{Gneiting2011,
abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, with the absolute error and the squared error being key examples. The individual scores are averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance. {\textcopyright} 2011 American Statistical Association.},
archivePrefix = {arXiv},
arxivId = {0912.0902},
author = {Gneiting, Tilmann},
doi = {10.1198/jasa.2011.r10138},
eprint = {0912.0902},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Bayes rule,Bregman function,Conditional value-at-risk (CVaR),Decision theory,Elicitability,Expectile,Mean,Median,Mode,Proper scoring rule,Quantile,Statistical functional},
month = {jun},
number = {494},
pages = {746--762},
title = {{Making and evaluating point forecasts}},
volume = {106},
year = {2011}
}
@article{Richards2008a,
abstract = {1. The ability to identify key ecological processes is important when solving applied problems. Increasingly, ecologists are adopting Akaike's information criterion (AIC) as a metric to help them assess and select among multiple process-based ecological models. Surprisingly, however, it is still unclear how best to incorporate AIC into the selection process in order to address the trade-off between maximizing the probability of retaining the most parsimonious model while minimizing the number of models retained. 2. Ecological count data are often observed to be overdispersed with respect to best-fitting models. Overdispersion is problematic when performing an AIC analysis, as it can result in selection of overly complex models which can lead to poor ecological inference. This paper describes and illustrates two approaches that deal effectively with overdispersion. The first approach involves modelling the causes of overdispersion implicitly using compound probability distributions. The second approach ignores the causes of overdispersion and uses quasi-AIC (QAIC) as a metric for model parsimony. 3. Simulations and a novel method that identifies the most parsimonious model are used to demonstrate the utility of the two overdispersion approaches within the context of two ecological examples. The first example addresses binomial data obtained from a study of fish survival (as related to habitat structure) and the second example addresses Poisson data obtained from a study of flower visitation by nectarivores. 4. Applying either overdispersion approach reduces the chance of selecting overly complex models, and both approaches result in very similar ecological inference. In addition, inference can be made more reliable by incorporating model nesting into the selection process (i.e. identifying which models are special cases of others), as it reduces the number of models selected without significantly reducing the probability of retaining the most parsimonious models. 5. Synthesis and applications. When data are overdispersed, inference can be improved by either modelling the causes of overdispersion or applying QAIC as a metric for model parsimony. Inference can also be improved by adopting a model filtering procedure based on how models are nested. The general simulation approach presented in this paper for identifying the most parsimonious model, as defined by information theory, should help to improve our understanding of the reliability of model selection when using AIC, and help the development of better selection rules. {\textcopyright} 2007 The Author.},
author = {Richards, Shane A.},
doi = {10.1111/j.1365-2664.2007.01377.x},
file = {::},
issn = {00218901},
journal = {Journal of Applied Ecology},
keywords = {AIC,Model selection,Overdispersion,QAIC,Quasi-likelihood},
month = {aug},
number = {1},
pages = {218--227},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Dealing with overdispersed count data in applied ecology}},
url = {http://doi.wiley.com/10.1111/j.1365-2664.2007.01377.x},
volume = {45},
year = {2008}
}
@article{ALLOUCHE2006,
abstract = {Totally, 294 bank voles (Clethrionomys glareolus) and 18 red-backed voles (C. rutilus) from 62 sites of European Russia were studied. Incomplete sequences (967 bp) of the mitochondrial cytochrome b gene were determined for 93 C. glareolus individuals from 56 sites and 18 C. rutilus individuals from the same habitats. Analysis of the cytochrome b gene variation has demonstrated that practically the entire European part of Russia, Ural, and a considerable part of Western Europe are inhabited by bank voles of the same phylogroup, displaying an extremely low genetic differentiation. Our data suggest that C. glareolus very rapidly colonized over the presently occupied territory in the post-Pleistocene period from no more than two (central European and western European) refugia for ancestral populations with a small efficient size. PCR typing of the mitochondrial cytochrome b gene allowed us to assess the scale of mtDNA introgression from a closely related species, C. rutilus, and to outline the geographical zone of this introgression. Comparison with the red-backed vole haplotypes in the habitats shared by both species favors the hypothesis of an ancient hybridization event (mid-Holocene) and a subsequent introgression. These results suggest that the hybridization took place in the southern and middle Pre-Ural region.},
author = {ALLOUCHE, OMRI and TSOAR, ASAF and KADMON, RONEN},
doi = {10.1111/j.1365-2664.2006.01214.x},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/ALLOUCHE{\_}et{\_}al-2006-Journal{\_}of{\_}Applied{\_}Ecology.pdf:pdf},
issn = {00218901},
journal = {Journal of Applied Ecology},
keywords = {AUC,Mahalanobis distance,ROC curves,predictive maps,sensitivity,specificity,woody plants},
month = {sep},
number = {6},
pages = {1223--1232},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Assessing the accuracy of species distribution models: prevalence, kappa and the true skill statistic (TSS)}},
url = {http://doi.wiley.com/10.1111/j.1365-2664.2006.01214.x},
volume = {43},
year = {2006}
}
@article{Shang2008,
abstract = {Two bootstrap-corrected variants of the Akaike information criterion are proposed for the purpose of small-sample mixed model selection. These two variants are asymptotically equivalent, and provide asymptotically unbiased estimators of the expected Kullback-Leibler discrepancy between the true model and a fitted candidate model. The performance of the criteria is investigated in a simulation study where the random effects and the errors for the true model are generated from a Gaussian distribution. The parametric bootstrap is employed. The simulation results suggest that both criteria provide effective tools for choosing a mixed model with an appropriate mean and covariance structure. A theoretical asymptotic justification for the variants is presented in the Appendix. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Shang, Junfeng and Cavanaugh, Joseph E},
doi = {10.1016/j.csda.2007.06.019},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shang, Cavanaugh - 2008 - Bootstrap variants of the Akaike information criterion for mixed model selection.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {AIC,Kullback-Leibler information,Model selection criteria},
number = {4},
pages = {2004--2021},
title = {{Bootstrap variants of the Akaike information criterion for mixed model selection}},
url = {http://myweb.uiowa.edu/cavaaugh/doc/pub/aicb{\_}mm.pdf},
volume = {52},
year = {2008}
}
@article{Colby2013,
abstract = {Cross-validation is frequently used for model selection in a variety of applications. However, it is difficult to apply cross-validation to mixed effects models (including nonlinear mixed effects models or NLME models) due to the fact that cross-validation requires "out-of-sample" predictions of the outcome variable, which cannot be easily calculated when random effects are present. We describe two novel variants of cross-validation that can be applied to NLME models. One variant, where out-of-sample predictions are based on post hoc estimates of the random effects, can be used to select the overall structural model. Another variant, where cross-validation seeks to minimize the estimated random effects rather than the estimated residuals, can be used to select covariates to include in the model. We show that these methods produce accurate results in a variety of simulated data sets and apply them to two publicly available population pharmacokinetic data sets. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Colby, Emily and Bair, Eric},
doi = {10.1007/s10928-013-9313-5},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Colby, Bair - 2013 - Cross-validation for nonlinear mixed effects models.pdf:pdf},
issn = {1567567X},
journal = {Journal of Pharmacokinetics and Pharmacodynamics},
keywords = {Cross-validation,Model selection,Nonlinear mixed effects,Population pharmacokinetic modeling},
month = {apr},
number = {2},
pages = {243--252},
title = {{Cross-validation for nonlinear mixed effects models}},
volume = {40},
year = {2013}
}
@article{Hurvich1989,
abstract = {A bias correction to the Akaike information criterion, AIC, is derived for regression and autoregressive time series models. The correction is of particular use when the sample size is small, or when the number of fitted parameters is a moderate to large fraction of the sample size. The corrected method, called AICC, is asymptotically efficient if the true model is infinite dimensional. Furthermore, when the true model is of finite dimension, AICC is found to provide better model order choices than any other asymptotically efficient method. Applications to nonstationary autoregressive and mixed autoregressive moving average time series models are also discussed. {\textcopyright} 1989 Biometrika Trust.},
author = {Hurvich, Clifford M. and Tsai, Chih Ling},
doi = {10.1093/biomet/76.2.297},
issn = {00063444},
journal = {Biometrika},
keywords = {AIC,Asymptotic efficiency,Kullback-Leibler information},
month = {jun},
number = {2},
pages = {297--307},
title = {{Regression and time series model selection in small samples}},
volume = {76},
year = {1989}
}
@article{Burnham2001,
abstract = {We describe an information-theoretic paradigm for analysis of ecological data, based on Kullback-Leibler information, that is an extension of likelihood theory and avoids the pitfalls of null hypothesis testing. Information-theoretic approaches emphasise a deliberate focus on the a priori science in developing a set of multiple working hypotheses or models. Simple methods then allow these hypotheses (models) to be ranked from best to worst and scaled to reflect a strength of evidence using the likelihood of each model (gi), given the data and the models in the set (i.e. L(gi | data)). In addition, a variance component due to model-selection uncertainty is included in estimates of precision. There are many cases where formal inference can be based on all the models in the a priori set and this multi-model inference represents a powerful, new approach to valid inference. Finally, we strongly recommend inferences based on a priori considerations be carefully separated from those resulting from some form of data dredging. An example is given for questions related to age- and sex-dependent rates of tag loss in elephant seals (Mirounga leonina).},
author = {Burnham, K. P. and Anderson, D. R.},
doi = {10.1071/WR99107},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burnham, Anderson - 2001 - Kullback-Leibler information as a basis for strong inference in ecological studies.pdf:pdf},
issn = {10353712},
journal = {Wildlife Research},
number = {2},
pages = {111--119},
title = {{Kullback-Leibler information as a basis for strong inference in ecological studies}},
volume = {28},
year = {2001}
}
@misc{Ohara2009,
abstract = {The selection of variables in regression problems has occupied the minds of many statisticians. Several Bayesian variable selection methods have been developed, and we concentrate on the following methods: Kuo {\&} Mallick, Gibbs Variable Selection (GVS), Stochastic Search Variable Selection (SSVS), adaptive shrinkage with Jeffreys' prior or a Laplacian prior, and reversible jump MCMC. We review these methods, in the context of their different properties. We then implement the methods in BUGS, using both real and simulated data as examples, and investigate how the different methods perform in practice. Our results suggest that SSVS, reversible jump MCMC and adaptive shrinkage methods can all work well, but the choice of which method is better will depend on the priors that are used, and also on how they are implemented. {\textcopyright} 2009 International Society for Bayesian Analysis.},
author = {O'Hara, R. B. and Sillanp{\"{a}}{\"{a}}, M J},
booktitle = {Bayesian Analysis},
doi = {10.1214/09-BA403},
file = {::},
issn = {19360975},
keywords = {BUGS,MCMC,Variable selection},
number = {1},
pages = {85--118},
title = {{A review of bayesian variable selection methods: What, how and which}},
url = {http://www.rni.},
volume = {4},
year = {2009}
}
@article{Anderson,
author = {Anderson, David R.},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/Alternative to T-Tests.pdf:pdf},
title = {{ALTERNATIVES TO A P -VALUE IN Unpaired Design :}},
url = {http://www.warnercnr.colostate.edu/{~}anderson/}
}
@article{Davies2005,
abstract = {For many situations, the predictive ability of a candidate model is its most important attribute. In light of our interest in this property, we introduce a new cross validation model selection criterion, the predictive divergence criterion (PDC), together with a description of the target discrepancy upon which it is based. In the linear regression framework, we then develop an adjusted cross validation model selection criterion (PDCa) which serves as the minimum variance unbiased estimator of this target discrepancy. Furthermore, we show that this adjusted criterion is asymptotically a minimum variance unbiased estimator of the Kullback-Leibler discrepancy which serves as the basis for the Akaike information criteria AIC and AICc. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Davies, Simon L. and Neath, Andrew A. and Cavanaugh, Joseph E.},
doi = {10.1016/j.stamet.2005.05.002},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/davies2005.pdf:pdf},
issn = {15723127},
journal = {Statistical Methodology},
keywords = {AIC,AICc,Akaike information criterion,Kullback-Leibler information},
month = {dec},
number = {4},
pages = {249--266},
publisher = {Elsevier},
title = {{Cross validation model selection criteria for linear regression based on the Kullback-Leibler discrepancy}},
url = {https://www.sciencedirect.com/science/article/abs/pii/S1572312705000237},
volume = {2},
year = {2005}
}
@article{Yang2007,
abstract = {Theoretical developments on cross validation (CV) have mainly focused on selecting one among a list of finite-dimensional models (e.g., subset or order selection in linear regression) or selecting a smoothing parameter (e.g., bandwidth for kernel smoothing). However, little is known about consistency of cross validation when applied to compare between parametric and nonparametric methods or within nonparametric methods. We show that under some conditions, with an appropriate choice of data splitting ratio, cross validation is consistent in the sense of selecting the better procedure with probability approaching 1. Our results reveal interesting behavior of cross validation. When comparing two models (procedures) converging at the same nonparametric rate, in contrast to the parametric case, it turns out that the proportion of data used for evaluation in CV does not need to be dominating in size. Furthermore, it can even be of a smaller order than the proportion for estimation while not affecting the consistency property. {\textcopyright} Institute of Mathematical Statistics, 2007.},
author = {Yang, Yuhong},
doi = {10.1214/009053607000000514},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang - 2007 - CONSISTENCY OF CROSS VALIDATION FOR COMPARING REGRESSION PROCEDURES 1.pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {CV bias-correction,Consistency,Cross validation,Model selection},
mendeley-tags = {CV bias-correction},
number = {6},
pages = {2450--2473},
title = {{Consistency of cross validation for comparing regression procedures}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.aos/1201012968},
volume = {35},
year = {2007}
}
@article{Vehtari2012,
abstract = {To date, several methods exist in the statistical literature for model assessment, which purport themselves specifically as Bayesian predictive methods. The decision theoretic assumptions on which these methods are based are not always clearly stated in the original articles, however. The aim of this survey is to provide a unified review of Bayesian predictive model assessment and selection methods, and of methods closely related to them. We review the various assumptions that are made in this context and discuss the connections between different approaches, with an emphasis on how each method approximates the expected utility of using a Bayesian model for the purpose of predicting future data. {\textcopyright} 2013 The author, under a Creative Commons Attribution License.},
author = {Vehtari, Aki and Ojanen, Janne},
doi = {10.1214/12-ss102},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vehtari, Ojanen - 2012 - A survey of Bayesian predictive methods for model assessment, selection and comparison †.pdf:pdf},
issn = {19357516},
journal = {Statistics Surveys},
keywords = {Bayesian,Cross-validation,Decision theory,Expected utility,Information criteria,Model assessment,Model selection,Predictive},
number = {1},
pages = {142--228},
title = {{A survey of Bayesian predictive methods for model assessment, selection and comparison}},
volume = {6},
year = {2012}
}


@article{Arlot2008,
abstract = {We study the efficiency of V-fold cross-validation (VFCV) for model selection from the non-asymptotic viewpoint, and suggest an improvement on it, which we call ``V-fold penalization''. Considering a particular (though simple) regression problem, we prove that VFCV with a bounded V is suboptimal for model selection, because it ``overpenalizes'' all the more that V is large. Hence, asymptotic optimality requires V to go to infinity. However, when the signal-to-noise ratio is low, it appears that overpenalizing is necessary, so that the optimal V is not always the larger one, despite of the variability issue. This is confirmed by some simulated data. In order to improve on the prediction performance of VFCV, we define a new model selection procedure, called ``V-fold penalization'' (penVF). It is a V-fold subsampling version of Efron's bootstrap penalties, so that it has the same computational cost as VFCV, while being more flexible. In a heteroscedastic regression framework, assuming the models to have a particular structure, we prove that penVF satisfies a non-asymptotic oracle inequality with a leading constant that tends to 1 when the sample size goes to infinity. In particular, this implies adaptivity to the smoothness of the regression function, even with a highly heteroscedastic noise. Moreover, it is easy to overpenalize with penVF, independently from the V parameter. A simulation study shows that this results in a significant improvement on VFCV in non-asymptotic situations.},
archivePrefix = {arXiv},
arxivId = {0802.0566},
author = {Arlot, Sylvain},
eprint = {0802.0566},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arlot - 2008 - V-FOLD CROSS-VALIDATION IMPROVED V-FOLD PENALIZATION.pdf:pdf},
keywords = {CV bias-correction},
mendeley-tags = {CV bias-correction},
title = {{V-fold cross-validation improved: V-fold penalization}},
url = {https://arxiv.org/pdf/0802.0566.pdf http://arxiv.org/abs/0802.0566},
year = {2008}
}
@book{Wood2015a,
abstract = {Based on a starter course for beginning graduate students, Core Statistics provides concise coverage of the fundamentals of inference for parametric statistical models, including both theory and practical numerical computation. The book considers both frequentist maximum likelihood and Bayesian stochastic simulation while focusing on general methods applicable to a wide range of models and emphasizing the common questions addressed by the two approaches. This compact package serves as a lively introduction to the theory and tools that a beginning graduate student needs in order to make the transition to serious statistical analysis: inference; modeling; computation, including some numerics; and the R language. Aimed also at any quantitative scientist who uses statistical methods, this book will deepen readers' understanding of why and when methods work and explain how to develop suitable methods for non-standard situations, such as in ecology, big data and genomics},
address = {Cambridge},
author = {Wood, Simon N.},
booktitle = {Core Statistics},
doi = {10.1017/CBO9781107741973},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Ebooks/coreStatistics{\_}WOOD.pdf:pdf},
isbn = {9781107741973},
pages = {1--250},
publisher = {Cambridge University Press},
title = {{Core statistics}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781107741973},
year = {2015}
}
@article{Burnham2004,
abstract = {The model selection literature has been generally poor at reflecting
the deep foundations of the Akaike information criterion (AIC) and
at making appropriate comparisons to the Bayesian information criterion
(BIC). There is a clear philosophy, a sound criterion based in information
theory, and a rigorous statistical foundation for AIC. AIC can be
justified as Bayesian using a “savvy” prior on models that is a function
of sample size and the number of model parameters. Furthermore, BIC
can be derived as a non-Bayesian result. Therefore, arguments about
using AIC versus BIC for model selection cannot be from a Bayes versus
frequentist perspective. The philosophical context of what is assumed
about reality, approximating models, and the intent of model-based
inference should determine whether AIC or BIC is used. Various facets
of such multimodel inference are presented here, particularly methods
of model averaging.},
author = {Burnham, Kenneth P. and Anderson, David R.},
doi = {10.1177/0049124104268644},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/Burnham and Anderson - Soc Behav 2004.pdf:pdf},
issn = {00491241},
journal = {Sociological Methods and Research},
keywords = {AIC,BIC,Model averaging,Model selection,Multimodel inference},
number = {2},
pages = {261--304},
title = {{Multimodel inference: Understanding AIC and BIC in model selection}},
volume = {33},
year = {2004}
}
@article{Garthwaite2010,
abstract = {We address the task of choosing prior weights for models that are to be used for weighted model averaging. Models that are very similar should usually be given smaller weights than models that are quite distinct. Otherwise, the importance of a model in the weighted average could be increased by augmenting the set of models with duplicates of the model or virtual duplicates of it. Similarly, the importance of a particular model feature (a certain covariate, say) could be exaggerated by including many models with that feature. Ways of forming a correlation matrix that reflects the similarity between models are suggested. Then, weighting schemes are proposed that assign prior weights to models on the basis of this matrix. The weighting schemes give smaller weights to models that are more highly correlated. Other desirable properties of a weighting scheme are identified, and we examine the extent to which these properties are held by the proposed methods. The weighting schemes are applied to real data, and prior weights, posterior weights and Bayesian model averages are determined. For these data, empirical Bayes methods were used to form the correlation matrices that yield the prior weights. Predictive variances are examined, as empirical Bayes methods can result in unrealistically small variances. {\textcopyright} 2010 Australian Statistical Publishing Association Inc.},
author = {Garthwaite, Paul H. and Mubwandarikwa, Emmanuel},
doi = {10.1111/j.1467-842X.2010.00589.x},
file = {::},
issn = {13691473},
journal = {Australian and New Zealand Journal of Statistics},
keywords = {Bayesian model averaging,Dilution priors,Model uncertainty,Prior weights},
month = {dec},
number = {4},
pages = {363--382},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Selection of weights for weighted model averaging}},
url = {http://doi.wiley.com/10.1111/j.1467-842X.2010.00589.x},
volume = {52},
year = {2010}
}
@misc{GRUEBER2011,
abstract = {Information theoretic approaches and model averaging are increasing in popularity, but this approach can be difficult to apply to the realistic, complex models that typify many ecological and evolutionary analyses. This is especially true for those researchers without a formal background in information theory. Here, we highlight a number of practical obstacles to model averaging complex models. Although not meant to be an exhaustive review, we identify several important issues with tentative solutions where they exist (e.g. dealing with collinearity amongst predictors; how to compute model-averaged parameters) and highlight areas for future research where solutions are not clear (e.g. when to use random intercepts or slopes; which information criteria to use when random factors are involved). We also provide a worked example of a mixed model analysis of inbreeding depression in a wild population. By providing an overview of these issues, we hope that this approach will become more accessible to those investigating any process where multiple variables impact an evolutionary or ecological response. {\textcopyright} 2011 The Authors. Journal of Evolutionary Biology {\textcopyright} 2011 European Society For Evolutionary Biology.},
author = {Grueber, C. E. and Nakagawa, S. and Laws, R. J. and Jamieson, I. G.},
booktitle = {Journal of Evolutionary Biology},
doi = {10.1111/j.1420-9101.2010.02210.x},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/GRUEBER2011.pdf:pdf},
issn = {1010061X},
keywords = {Akaike Information Criterion,Generalized linear mixed models,Inbreeding,Information theory,Lethal equivalents,Model averaging,Random factors,Standardized predictors},
month = {apr},
number = {4},
pages = {699--711},
pmid = {21272107},
title = {{Multimodel inference in ecology and evolution: Challenges and solutions}},
url = {http://doi.wiley.com/10.1111/j.1420-9101.2010.02210.x},
volume = {24},
year = {2011}
}
@article{Breiman1996,
abstract = {Stacking regressions is a method for forming linear combinations of different predictors to give improved prediction accuracy. The idea is to use cross-validation data and least squares under non-negativity constraints to determine the coefficients in the combination. Its effectiveness is demonstrated in stacking regression trees of different sizes add in a simulation stacking linear subset and ridge regressions. Reasons why this method works are explored. The idea of stacking originated with Wolpert (1992)},
author = {Breiman, Leo},
doi = {10.1007/bf00117832},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman - 1996 - Stacked regressions.pdf:pdf},
issn = {0885-6125},
journal = {Machine Learning},
month = {jul},
number = {1},
pages = {49--64},
publisher = {Kluwer Academic Publishers},
title = {{Stacked regressions}},
url = {http://link.springer.com/10.1007/BF00117832},
volume = {24},
year = {2004}
}
@article{Breiman1992,
abstract = {Often, in a regression situation with many variables, a sequence of submodels is generated containing fewer variables by using such methods as stepwise addition or deletion of variables, or 'best subsets'. The question is which of this sequence of submodels is 'best', and how can submodel performance be evaluated. This was explored in Breiman (1988) for a fixed X-design. This is a sequel exploring the case of random X-designs. Analytical results are difficult, if not impossible. This study involved an extensive simulation. The basis of the study is the theoretical definition of prediction error (PE) as the expected squared error produced by applying a prediction equation to the distributional universe of (y, x) values. This definition is used throughout to compare various submodels. There can be startling differences between the x-fixed and x-random situations and different PE estimates are appropriate. Non-resampling estimates such as CP, adjusted R2, etc. turn out to be highly biased methods for submodel selection. The two best methods are cross-validation and bootstrap. One surprise is that 5 fold cross-validation (leave out 20{\%} of the data) is better at submodel selection and evaluation than leave-one-out cross-validation. There are a number of other surprises. /// Dans l'analyse de probl{\`{e}}mes de r{\'{e}}gression {\`{a}} plusieurs variables (ind{\'{e}}pendantes), on produit souvent une s{\'{e}}rie de sous-mod{\`{e}}les constitu{\'{e}}s d'un sous-ensemble des variables par des m{\'{e}}thodes telles que l'addition par {\'{e}}tape, le retrait par {\'{e}}tape et la m{\'{e}}thode du meilleur sous-ensemble. Le probl{\`{e}}me est de d{\'{e}}terminer lequel de ces sous-mod{\`{e}}les est le meilleur et d'{\'{e}}valuer sa performance. Ce probl{\`{e}}me fut explor{\'{e}} dans Breiman (1988) pour le cas d'une matrice X fixe. Dans ce qui suit on consid{\`{e}}re le cas o{\`{u}} la matrice X est al{\'{e}}atoire. La d{\'{e}}termination de r{\'{e}}sultats analytiques est difficile, sinon impossible. Notre {\'{e}}tude a utilis{\'{e}} des simulations de grande envergure. Elle se base sur la d{\'{e}}finition th{\'{e}}orique de l'erreur de pr{\'{e}}diction (EP) comme {\'{e}}tant l'esp{\'{e}}rance du carr{\'{e}} de l'erreur produite en applicant une {\'{e}}quation de pr{\'{e}}diction {\`{a}} l'univers distributional des valeurs (y, x). La d{\'{e}}finition est utilis{\'{e}}e dans toute l'{\'{e}}tude {\`{a}} fin de comparer divers sous-mod{\`{e}}les. Il y a une diff{\'{e}}rence {\'{e}}tonnante entre le cas o{\`{u}} la matrice X est fix{\'{e}}e et celui o{\`{u}} elle est al{\'{e}}atoire. Diff{\'{e}}rents estimateurs de la EP sont {\`{a}} propos. Les estimateurs n'utilisant pas de r{\'{e}}-{\'{e}}chantillonage, tels que le Cp et le R2 ajust{\'{e}}, produisent des m{\'{e}}thodes de s{\'{e}}lection ayant grand biais. Les deux meilleures m{\'{e}}thodes sont la validation crois{\'{e}}e et l'autoamor{\c{c}}age. Une surprise est que la validation crois{\'{e}}e quintuple est meilleure que la validation crois{\'{e}}e tous sauf un. Il y a plusieurs autres r{\'{e}}sultats surprenants.},
author = {Breiman, Leo and Spector, Philip},
doi = {10.2307/1403680},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Breiman, Spector - 1992 - Submodel Selection and Evaluation in Regression. The X-Random Case.pdf:pdf},
issn = {03067734},
journal = {International Statistical Review / Revue Internationale de Statistique},
number = {3},
pages = {291},
title = {{Submodel Selection and Evaluation in Regression. The X-Random Case}},
url = {https://www.jstor.org/stable/pdf/1403680.pdf?refreqid=excelsior{\%}3A4c05fa3ba812aa561cce843038a5e069},
volume = {60},
year = {1992}
}
@article{Mcculloch1989,
abstract = {This article develops a general method for assessing the influence of model assumptions in a Bayesian analysis. We assume that model choices are indexed by a hyperparameter with some given initial choice. We use the term “model” to encompass both the sampling model and the prior distribution. We wish to assess the effect of changing the hyperparameter away from the initial choice. We are performing a sensitivity analysis, with the hyperparameter defining our perturbations. We use the Kullback—Leibler divergence to measure the difference between posteriors corresponding to different choices of the hyperparameter. We also measure the change in priors. If small changes in the priors lead to large changes in posteriors, the choice of hyperparameter is influential. The second-order difference in the Kullback—Leibler divergence is expressed by Fisher information matrices. The relative change in posteriors compared with priors may be summarized by the relative eigenvalue of the posterior and prior Fisher information matrices. The corresponding eigenvector indicates which aspects of the perturbation hyperparameter are most influential. Examples considered are the choice of conjugate prior in regression, case weights in regression, and the choice of Dirichlet prior for multinomials. {\textcopyright} 1989 Taylor {\&} Francis Group, LLC.},
author = {McCulloch, Robert E.},
doi = {10.1080/01621459.1989.10478793},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Diagnostics,Fisher information,Kullback—Leibler divergence,Posterior distribution,Predictive distribution},
number = {406},
pages = {473--478},
title = {{Local model influence}},
volume = {84},
year = {1989}
}
@article{Breiman1996a,
abstract = {In model selection, usually a "best" predictor is chosen from a collec-tion {\{}$\mu$̂({\textperiodcentered}, s){\}} of predictors where $\mu$({\textperiodcentered}, s) is the minimum least-squares predictor in a collection script U signs of predictors. Here s is a complexity parameter; that is, the smaller s, the lower dimensional/smoother the models in script U signs. If ℒ is the data used to derive the sequence {\{}$\mu$̂({\textperiodcentered}, s){\}}, the procedure is called unstable if a small change in ℒ can cause large changes in {\{}$\mu$̂({\textperiodcentered}, s){\}}. With a crystal ball, one could pick the predictor in {\{}$\mu$̂({\textperiodcentered}, s){\}} having minimum prediction error. Without prescience, one uses test sets, cross-validation and so forth. The difference in prediction error between the crystal ball selection and the statistician's choice we call predictive loss. For an unstable procedure the predictive loss is large. This is shown by some analytics in a simple case and by simulation results in a more complexcomparison of four different linear regression methods. Unstable procedures can be stabilized by perturbing the data, getting a new predictor sequence {\{}$\mu$̂′({\textperiodcentered}, s){\}} and then averaging over many such predictor sequences.},
author = {Breiman, Leo},
doi = {10.1214/aos/1032181158},
file = {::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Cross-validation,Prediction error,Predictive loss,Regression,Subset selection},
number = {6},
pages = {2350--2383},
title = {{Heuristics of instability and stabilization in model selection}},
volume = {24},
year = {1996}
}
@article{Cavanaugh1997,
abstract = {The Akaike (1973, 1974) information criterion, AIC, and the corrected Akaike information criterion (Hurvich and Tsai, 1989), AICc, were both designed as estimators of the expected Kullback-Leibler discrepancy between the model generating the data and a atted candidate model. AIC is justiied in a very general framework, and as a result, ooers a crude estimator of the expected discrepancy: one which exhibits a potentially high degree of negative bias in small-sample applications (Hurvich and Tsai, 1989). AICc corrects for this bias, but is less broadly applicable than AIC since its justiication depends upon the form of the candidate model (Hurvich and Tsai, 1989, 1993; Hurvich, Shumway, and Tsai, 1990; Bedrick and Tsai, 1994). Although AIC and AICc share the same objective, the derivations of the criteria proceed along very diierent lines, making it diicult to reconcile how AICc improves upon the approximations leading to AIC. To address this issue, we present a derivation which uniies the justiications of AIC and AICc in the linear regression framework.},
author = {Cavanaugh, Joseph E},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cavanaugh - 1997 - Unifying the Derivations for the Akaike and Corrected Akaike Information Criteria from Statistics {\&}amp Probability Le.pdf:pdf},
journal = {Statistics {\&} Probability Letters},
keywords = {AIC,AICc,Kullback-Leibler information,information theory,model selection},
pages = {201--208},
title = {{Unifying the Derivations for the Akaike and Corrected Akaike Information Criteria from Statistics {\&} Probability Letters}},
url = {https://pdfs.semanticscholar.org/344f/c47f9909a86f0a451ca91361383e847b4aa0.pdf},
volume = {33},
year = {1997}
}
@article{Stone1974,
abstract = {A generalized form of the cross-validation criterion is applied to the choice and assessment of prediction using the data-analytic concept of a prescrip- tion. The examples used to illustrate the application are drawn from the problem areas of univariate estimation, linear regression and analysis of variance.},
author = {Stone, M},
doi = {10.1111/j.2517-6161.1976.tb01573.x},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stone - 1974 - Cross-Validatory Choice and Assessment of Statistical Predictions.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {1},
pages = {102--102},
title = {{Cross-Validatory Choice and Assessment of Statistical Predictions (With Discussion)}},
url = {https://www.jstor.org/stable/pdf/2984809.pdf?ab{\_}segments=0{\%}2Fdefault-2{\%}2Fcontrol{\&}refreqid=search{\%}3A84e965d24296db678518872b8e67b4d1},
volume = {38},
year = {1976}
}
@book{Maindonald2009,
author = {Maindonald, John H.},
booktitle = {International Statistical Review},
doi = {10.1111/j.1751-5823.2009.00095_16.x},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/Anderson 2008 Information theory book.pdf:pdf},
isbn = {9780387740737},
issn = {03067734},
number = {3},
pages = {479--480},
title = {{Model Based Inference in the Life Sciences: A Primer on Evidence by David R. Anderson}},
volume = {77},
year = {2009}
}
@article{Fang2011,
abstract = {For model selection in mixed effects models, Vaida and Blan-chard (2005) demonstrated that the marginal Akaike information criterion is appropriate as to the questions regarding the population and the con-ditional Akaike information criterion is appropriate as to the questions re-garding the particular clusters in the data. This article shows that the marginal Akaike information criterion is asymptotically equivalent to the leave-one-cluster-out cross-validation and the conditional Akaike informa-tion criterion is asymptotically equivalent to the leave-one-observation-out cross-validation.},
author = {Fang, Yixin},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Fang2011.pdf:pdf},
journal = {Journal of Data Science},
keywords = {AIC,degrees of freedom,functional data,model selection},
pages = {15--21},
title = {{Asymptotic Equivalence between Cross-Validations and Akaike Information Criteria in Mixed-Effects Models}},
volume = {9},
year = {2011}
}
@article{Breiman1992a,
abstract = {When a regression problem contains many predictor variables, it is rarely wise to try to fit the data by means of a least squares regression on all of the predictor variables. Usually, a regression equation based on a few variables will be more accurate and certainly simpler. There are various methods for picking “good” subsets of variables, and programs that do such procedures are part of every widely used statistical package. The most common methods are based on stepwise addition or deletion of variables and on “best subsets.” The latter refers to a search method that, given the number of variables to be in the equation (say, five), locates that regression equation based on five variables that has the lowest residual sum of squares among all five variable equations. All of these procedures generate a sequence of regression equations, the first based on one variable, the next based on two variables, and so on. Each member of this sequence is called a submodel and the number of variables in the equation is the dimensionality of the submodel. A complex problem is determining which submodel of the generated sequence to select. Statistical packages use various ad hoc selection methods, including F to enter, F to delete, Cp, and t-value cutoffs. Our approach to this problem is through the criterion that a good selection procedure selects dimensionality so as to give low prediction error (PE), where the PE of a regression equation is its expected squared error over the points in the X design. Because the true PE is unknown, the use of this criteria must be based on PE estimates. We introduce a method called the little bootstrap, which gives almost unbiased estimates for submodel PEs and then uses these to do submodel selection. Comparison is made to Cp and other methods by analytic examples and simulations. Little bootstrap does well—Cp and, by implication, all selection methods not based on data reuse give highly biased results and poor subset selection. {\textcopyright} 1992 Taylor {\&} Francis Group, LLC.},
author = {Breiman, Leo},
doi = {10.1080/01621459.1992.10475276},
file = {::},
issn = {1537274X},
journal = {Journal of the American Statistical Association},
keywords = {Best subsets,Mallows's Cp,Subset selection,Variable selection},
number = {419},
pages = {738--754},
title = {{The little bootstrap and other methods for dimensionality selection in regression: X-fixed prediction error}},
volume = {87},
year = {1992}
}
@article{Hooten2015,
abstract = {The steady upward trend in the use of model selection and Bayesian methods in ecological research has made it clear that both approaches to inference are important for modern analysis of models and data. However, in teaching Bayesian methods and in working with our research colleagues, we have noticed a general dissatisfaction with the available literature on Bayesian model selection and multimodel inference. Students and researchers new to Bayesian methods quickly find that the published advice on model selection is often preferential in its treatment of options for analysis, frequently advocating one particular method above others. The recent appearance of many articles and textbooks on Bayesian modeling has provided welcome background on relevant approaches to model selection in the Bayesian framework, but most of these are either very narrowly focused in scope or inaccessible to ecologists. Moreover, the methodological details of Bayesian model selection approaches are spread thinly throughout the literature, appearing in journals from many different fields. Our aim with this guide is to condense the large body of literature on Bayesian approaches to model selection and multimodel inference and present it specifically for quantitative ecologists as neutrally as possible. We also bring to light a few important and fundamental concepts relating directly to model selection that seem to have gone unnoticed in the ecological literature. Throughout, we provide only a minimal discussion of philosophy, preferring instead to examine the breadth of approaches as well as their practical advantages and disadvantages. This guide serves as a reference for ecologists using Bayesian methods, so that they can better understand their options and can make an informed choice that is best aligned with their goals for inference.},
author = {Hooten, M. B. and Hobbs, N. T. and Ellison, A. M.},
doi = {10.1890/14-0661.1},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hooten, Hobbs - 2015 - A guide to Bayesian model selection for ecologists.pdf:pdf},
issn = {15577015},
journal = {Ecological Monographs},
keywords = {Akaike information criterion,Bayes factors,Cross-validation,Deviance information criterion,Model averaging,Multi-model inference,Regularization,Shrinkage},
month = {feb},
number = {1},
pages = {3--28},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{A guide to Bayesian model selection for ecologists}},
url = {http://doi.wiley.com/10.1890/14-0661.1},
volume = {85},
year = {2015}
}
@article{Breiman1996b,
abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. {\textcopyright} 1996 Kluwer Academic Publishers,.},
author = {Breiman, Leo},
doi = {10.1007/bf00058655},
file = {::},
issn = {08856125},
journal = {Machine Learning},
keywords = {Aggregation,Averaging,Bootstrap,Combining},
month = {aug},
number = {2},
pages = {123--140},
publisher = {Kluwer Academic Publishers},
title = {{Bagging predictors}},
url = {http://link.springer.com/10.1007/BF00058655},
volume = {24},
year = {1996}
}
@article{Jonathan2000,
abstract = {We describe a Monte Carlo investigation of a number of variants of cross-validation for the assessment of performance of predictive models, including different values of k in leave-k-out cross-validation, and implementation either in a one-deep or a two-deep fashion. We assume an underlying linear model that is being fitted using either ridge regression or partial least squares, and vary a number of design factors such as sample size n relative to number of variables p, and error variance. The investigation encompasses both the non-singular (i.e. n {\textgreater} p) and the singular (i.e. n ≤ p) cases. The latter is now common in areas such as chemometrics but has as yet received little rigorous investigation. Results of the experiments enable us to reach some definite conclusions and to make some practical recommendations.},
author = {Jonathan, P and Krzanowski, W J and McCarthy, W. V.},
doi = {10.1023/A:1008987426876},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Jonathan, Krzanowski, Mccarthy - 2000 - On the use of cross-validation to assess performance in multivariate prediction.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Assessment of predictive models,Cross-validation,Partial least squares,Prediction,Ridge regression},
number = {3},
pages = {209--229},
title = {{On the use of cross-validation to assess performance in multivariate prediction}},
url = {https://link.springer.com/content/pdf/10.1023{\%}2FA{\%}3A1008987426876.pdf},
volume = {10},
year = {2000}
}
@article{Cawley2010,
abstract = {Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a nonnegligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds. {\textcopyright} 2010 Gavin C. Cawley and Nicola L. C. Talbot.},
author = {Cawley, Gavin C and Talbot, Nicola L.C.},
file = {::},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Bias-variance trade-off,Model selection,Overfitting,Performance evaluation,Selection bias},
pages = {2079--2107},
title = {{On over-fitting in model selection and subsequent selection bias in performance evaluation}},
volume = {11},
year = {2010}
}
@article{Brewer2016,
abstract = {Model selection is difficult. Even in the apparently straightforward case of choosing between standard linear regression models, there does not yet appear to be consensus in the statistical ecology literature as to the right approach. We review recent works on model selection in ecology and subsequently focus on one aspect in particular: the use of the Akaike Information Criterion (AIC) or its small-sample equivalent, AICC. We create a novel framework for simulation studies and use this to study model selection from simulated data sets with a range of properties, which differ in terms of degree of unobserved heterogeneity. We use the results of the simulation study to suggest an approach for model selection based on ideas from information criteria but requiring simulation. We find that the relative predictive performance of model selection by different information criteria is heavily dependent on the degree of unobserved heterogeneity between data sets. When heterogeneity is small, AIC or AICC are likely to perform well, but if heterogeneity is large, the Bayesian Information Criterion (BIC) will often perform better, due to the stronger penalty afforded. Our conclusion is that the choice of information criterion (or more broadly, the strength of likelihood penalty) should ideally be based upon hypothesized (or estimated from previous data) properties of the population of data sets from which a given data set could have arisen. Relying on a single form of information criterion is unlikely to be universally successful.},
author = {Brewer, Mark J. and Butler, Adam and Cooksley, Susan L.},
doi = {10.1111/2041-210X.12541},
editor = {Freckleton, Robert},
file = {::},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Akaike Information Criterion,Bayesian Information Criterion,generalized linear models,likelihood penalization,linear regression,model selection,statistical controversies},
month = {jun},
number = {6},
pages = {679--692},
publisher = {British Ecological Society},
title = {{The relative performance of AIC, AICC and BIC in the presence of unobserved heterogeneity}},
url = {http://doi.wiley.com/10.1111/2041-210X.12541},
volume = {7},
year = {2016}
}
@article{Burman1989,
abstract = {Concepts of v-fold cross validation and repeated learning-testing methods have been introduced here. In many problems, these methods are computationally much less expensive than ordinary cross-validation and can be used in its place. A comparative study of these three methods has been carried out in detail.},
author = {Burman, Prabir},
doi = {10.1093/biomet/76.3.503},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Burman(1989).pdf:pdf},
issn = {0006-3444},
journal = {Biometrika},
month = {sep},
number = {3},
pages = {503--514},
title = {{A comparative study of ordinary cross-validation, v-fold cross-validation and the repeated learning-testing methods}},
url = {https://doi.org/10.1093/biomet/76.3.503},
volume = {76},
year = {1989}
}
@article{Vehtari2002,
abstract = {In this work, we discuss practical methods for the assessment, comparison, and selection of complex hierarchical Bayesian models. A natural way to assess the goodness of the model is to estimate its future predictive capability by estimating expected utilities. Instead of just making a point estimate, it is important to obtain the distribution of the expected utility estimate because it describes the uncertainty in the estimate. The distributions of the expected utility estimates can also be used to compare models, for example, by computing the probability of one model having a better expected utility than some other model. We propose an approach using cross-validation predictive densities to obtain expected utility estimates and Bayesian bootstrap to obtain samples from their distributions. We also discuss the probabilistic assumptions made and properties of two practical cross-validation methods, importance sampling and k-fold cross-validation. As illustrative examples, we use multilayer perceptron neural networks and gaussian processes with Markov chain Monte Carlo sampling in one toy problem and two challenging real-world problems.},
author = {Vehtari, Aki and Lampinen, Jouko},
doi = {10.1162/08997660260293292},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/vehtari2002.pdf:pdf},
issn = {08997667},
journal = {Neural Computation},
month = {oct},
number = {10},
pages = {2439--2468},
title = {{Bayesian model assessment and comparison using cross-validation predictive densities}},
volume = {14},
year = {2002}
}
@article{Bahn2007,
abstract = {Aim: Distribution modelling relates sparse data on species occurrence or abundance to environmental information to predict the population of a species at any point in space. Recently, the importance of spatial autocorrelation in distributions has been recognized. Spatial autocorrelation can be categorized as exogenous (stemming from autocorrelation in the underlying variables) or endogenous (stemming from activities of the organism itself, such as dispersal). Typically, one asks whether spatial models explain additional variability (endogenous) in comparison to a fully specified habitat model. We turned this question around and asked: can habitat models explain additional variation when spatial structure is accounted for in a fully specified spatially explicit model? The aim was to find out to what degree habitat models may be inadvertently capturing spatial structure rather than true explanatory mechanisms. Location: We used data from 190 species of the North American Breeding Bird Survey covering the conterminous United States and southern Canada. Methods: We built 13 different models on 190 bird species using regression trees. Our habitat-based models used climate and landcover variables as independent variables. We also used random variables and simulated ranges to validate our results. The two spatially explicit models included only geographical coordinates or a contagion term as independent variables. As another angle on the question of mechanism vs. spatial structure we pitted a model using related bird species as predictors against a model using randomly selected bird species. Results: The spatially explicit models outperformed the traditional habitat models and the random predictor species outperformed the related predictor species. In addition, environmental variables produced a substantial R 2 in predicting artificial ranges. Main conclusions: We conclude that many explanatory variables with suitable spatial structure can work well in species distribution models. The predictive power of environmental variables is not necessarily mechanistic, and spatial interpolation can outperform environmental explanatory variables. {\textcopyright} 2007 The AuthorsJournal compilation {\textcopyright} 2007 Blackwell Publishing Ltd.},
author = {Bahn, Volker and Mcgill, Brian J.},
doi = {10.1111/j.1466-8238.2007.00331.x},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Bahn{\_}et{\_}al-2007-Global{\_}Ecology{\_}and{\_}Biogeography.pdf:pdf},
issn = {1466822X},
journal = {Global Ecology and Biogeography},
keywords = {Birds,Distribution modelling,Habitat,Macroecology,Model evaluation,Neighbourhood,Niche,Spatialautocorrelation,Spatialinterpolation,Speciesdistributions},
month = {nov},
number = {6},
pages = {733--742},
title = {{Can niche-based distribution models outperform spatial interpolation?}},
url = {http://doi.wiley.com/10.1111/j.1466-8238.2007.00331.x},
volume = {16},
year = {2007}
}
@article{Shen2002,
abstract = {Most model selection procedures use a fixed penalty penalizing an increase in the size of a model. These nonadaptive selection procedures perform well only in one type of situation. For instance, Bayesian information criterion (BIC) with a large penalty performs well for "small" models and poorly for "large" models, and Akaike's information criterion (AIC) does just the opposite. This article proposes an adaptive model selection procedure that uses a data-adaptive complexity penalty based on a concept of generalized degrees of freedom. The proposed procedure, combining the benefit of a class of nonadaptive procedures, approximates the best performance Of this class of procedures across a variety of different situations. This class includes many well-known procedures, such as AIC, BIC, Mallows's C p, and risk inflation criterion (RIC). The proposed procedure is applied to wavelet thresholding in nonparametric regression and variable selection in least squares regression. Simulation results and an asymptotic analysis support the effectiveness of the proposed procedure.},
author = {Shen, X. and Ye, J.},
doi = {10.1198/016214502753479356},
file = {::},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Adaptive penalty,False discovery rate,Optimal predication,Parametric and nonparametric regression,Variable selection,Wavelets},
month = {mar},
number = {457},
pages = {210--221},
publisher = {Taylor {\&} Francis},
title = {{Adaptive model selection}},
volume = {97},
year = {2002}
}
@article{Hodges2001,
abstract = {Drawing on linear model theory, we rigorously extend the notion of degrees of freedom to richly-parameterised models, including linear hierarchical and random-effect models, some smoothers and spatial models, and combinations of these. The number of degrees of freedom is often much smaller than the number of parameters. Our notion of degrees of freedom is compatible with similar ideas long associated with smoothers, but is applicable to new classes of models and can be interpreted using the projection theory of linear models. We use an example to illustrate the two applications of setting prior distributions for variances and fixing model complexity by fixing degrees of freedom. {\textcopyright} 2001 Biometrika Trust.},
author = {Hodges, James S. and Sargent, Daniel J.},
doi = {10.1093/biomet/88.2.367},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hodges2001.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Complexity,Degrees of freedom,Hierarchical model,Prior distribution,Random-effect model,Smoothing},
month = {jun},
number = {2},
pages = {367--379},
title = {{Counting degrees of freedom in hierarchical and other richly-parameterised models}},
url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/88.2.367},
volume = {88},
year = {2001}
}
@book{Efron1994,
author = {Efron, Bradley and Tibshirani, R.J.},
booktitle = {An Introduction to the Bootstrap},
doi = {10.1201/9780429246593},
month = {may},
publisher = {Chapman and Hall/CRC},
title = {{An Introduction to the Bootstrap}},
url = {https://www.taylorfrancis.com/books/9780429246593},
year = {1994}
}
@misc{Evans2013,
abstract = {Modellers of biological, ecological, and environmental systems cannot take for granted the maxim 'simple means general means good'. We argue here that viewing simple models as the main way to achieve generality may be an obstacle to the progress of ecological research. We show how complex models can be both desirable and general, and how simple and complex models can be linked together to produce broad-scale and predictive understanding of biological systems. {\textcopyright} 2013 Elsevier Ltd.},
author = {Evans, Matthew R. and Grimm, Volker and Johst, Karin and Knuuttila, Tarja and de Langhe, Rogier and Lessells, Catherine M. and Merz, Martina and O'Malley, Maureen A. and Orzack, Steve H. and Weisberg, Michael and Wilkinson, Darren J. and Wolkenhauer, Olaf and Benton, Tim G.},
booktitle = {Trends in Ecology and Evolution},
doi = {10.1016/j.tree.2013.05.022},
file = {::},
issn = {01695347},
month = {oct},
number = {10},
pages = {578--583},
pmid = {23827437},
publisher = {Elsevier Current Trends},
title = {{Do simple models lead to generality in ecology?}},
volume = {28},
year = {2013}
}
@article{Devi2014,
abstract = {Novel eco-friendly methods for the synthesis of metal nanoparticles are preferred and have recently been widely researched. In this study, a cost-effective and environmentally friendly method for the synthesis of gold nanoparticles was investigated. Costus pictus, also known as insulin plant due to its antidiabetic activity, was used as the reducing agent. Chloroaurate ions were rapidly reduced by the methanolic extract of Costus pictus, which was indicated by the color change from yellow to ruby red. The presence of a peak at 530 nm in UV-visible spectrophotometry confirmed the reduction of chloroaurate to gold nanoparticles. The reaction kinetics and effects of time, temperature, concentration of extract and substrate were also studied. The size and morphology of gold nanoparticles were analyzed using scanning electron microscopy (SEM) and high-resolution transmission electron microscopy (HR-TEM) and they were found to be cuboidal, with a size of around 20 nm. The size of the particles was also analyzed by the particle size analyzer, and was found to be 45 nm. These particles were functionalized with bisdemethoxycurcumin analog (BDMCA). X-ray diffraction (XRD) analysis of prepared nanoparticles showed that the gold nanoparticles have a face centered cubic structure and that of functionalized nanoparticles showed peaks similar to that of BDMCA. Fourier transform infrared spectroscopy (FTIR) studies revealed the presence of reducing groups in the plant methanolic extract and the functionalization of nanoparticles with BDMCA was also confirmed. The 3-(4,5-dimethylthiazol-2-yl)-2,5-diphenyltetrazolium bromide (MTT) assay showed that, the BDMCA-functionalized gold nanocubes possess anticancer activity. The mechanism of action is discussed.},
author = {Devi, Rajagopal Aruna and Francis, Arul Prakash and Devasena, Thiyagarajan},
doi = {10.1890/15-1471.1},
issn = {21919550},
journal = {Green Processing and Synthesis},
keywords = {BDMCA,Costus pictus,Cytotoxicity,Gold nanocubes,Green synthesis},
month = {jul},
number = {1},
pages = {47--61},
title = {{Green-synthesized gold nanocubes functionalized with bisdemethoxycurcumin analog as an ideal anticancer candidate}},
url = {http://doi.wiley.com/10.1890/15-1471.1},
volume = {3},
year = {2014}
}
@article{Shao1993,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. We consider the problem of selecting a model having the best predictive ability among a class of linear models. The popular l one-out cross-validation method, which is asymptotically equivalent to many other model selection methods such as the Akaik information criterion (AIC), the Cp, and the bootstrap, is asymptotically inconsistent in the sense that the probability of selec the model with the best predictive ability does not converge to 1 as the total number of observations n-s o. We show that t inconsistency of the leave-one-out cross-validation can be rectified by using a leave-n,-out cross-validation with nv, the numbe observations reserved for validation, satisfying no/n-1 I as n s* xoo. This is a somewhat shocking discovery, because ne/n-* totally opposite to the popular leave-one-out recipe in cross-validation. Motivations, justifications, and discussions of some prac aspects of the use of the leave-n,-out cross-validation method are provided, and results from a simulation study are presented.},
author = {Shao, Jun},
doi = {10.1016/j.jspi.2003.10.004},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shao - 1993 - Linear Model Selection by Cross-Validation.pdf:pdf},
issn = {0378-3758},
journal = {Journal of Statistical Planning and Inference},
keywords = {AIC,BIC,Consistency,Cross-validation,GIC,Linear regression,Model selection,Monte Carlo,Variables selection},
number = {1},
pages = {231--240},
title = {{Linear model selection by cross-validation}},
url = {https://www.jstor.org/stable/pdf/2290328.pdf?ab{\_}segments=0{\%}2Fdefault-2{\%}2Fcontrol{\&}refreqid=search{\%}3Afc0c32f3f3da7148f223ac8e02b87a24},
volume = {128},
year = {1993}
}
@article{Hoeting1999,
abstract = {Standard statistical practice ignores model uncertainty. Data analysts typically select a model from some class of models and then proceed as if the selected model had generated the data. This approach ignores the uncertainty in model selection, leading to over-confident inferences and decisions that are more risky than one thinks they are. Bayesian model averaging (BMA) provides a coherent mechanism for accounting for this model uncertainty. Several methods for implementing BMA have recently emerged. We discuss these methods and present a number of examples. In these examples, BMA provides improved out-ofsample predictive performance. We also provide a catalogue of currently available BMA software. {\textcopyright} 1999 Institute of Mathematical Statistics.},
author = {Hoeting, Jennifer A and Madigan, David and Raftery, Adrian E and Volinsky, Chris T},
doi = {10.1214/ss/1009212519},
file = {::},
issn = {08834237},
journal = {Statistical Science},
keywords = {Bayesian graphical models,Bayesian model averaging,Learning,Markov chain Monte Carlo,Model uncertainty},
number = {4},
pages = {382--401},
title = {{Bayesian model averaging: A tutorial}},
volume = {14},
year = {1999}
}
@article{Picard1984,
abstract = {06:24 UTC JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. A methodolgy for assessment of the predictive ability of regression models is presented. Attention is given to models obtained via subset selection procedures, which are extremely difficult to evaluate by standard techniques. Cross-validatory assessments of predictive ability are obtained and their use illustrated in examples.},
author = {Taylor, Publisher and Picard, Richard R and Cook, R Dennis},
doi = {10.1080/01621459.1984.10478083},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Picard, Dennis, Cook - 1984 - Cross-Validation of Regression Models.pdf:pdf},
isbn = {01621459},
issn = {0162-1459},
journal = {Cook Source: Journal of the American Statistical Association},
keywords = {data splitting,model selection,optimism principle,predic-,tion},
number = {April 2013},
pages = {37--41},
title = {{Cross-Validation of Regression Models Cross-Validation of Regression Models}},
url = {https://www.jstor.org/stable/pdf/2288403.pdf?ab{\_}segments=0{\%}252Fdefault-2{\%}252Fcontrol{\&}refreqid=excelsior{\%}3A5f6473e6b8c7f34c53769f81ba8b8652},
volume = {79},
year = {2012}
}
@article{Shibata1997,
abstract = {Abstract: Estimation of Kullback-Leibler information is a crucial part of deriving a statistical model selection procedure which, like AIC, is based on the likelihood principle. To discriminate between nested models, we have to estimate Kullback- Leibler information up to the order of a constant, while Kullback-Leibler informa- tion itself is of the order of the number of observations. A correction term employed in AIC is an example of how to fulfill this requirement; however the correction is a simple minded bias correction to the log maximum likelihood and there is no assurance that such a bias correction yields a good estimate of Kullback-Leibler information. In this paper we investigate a bootstrap type estimate of Kullback- Leibler information as an alternative. We first show that both bootstrap estimates proposed by Efron (1983, 1986) and by Cavanaugh and Shumway (1997) are at least asymptotically equivalent and there exist many other equivalent bootstrap estimates. We also show that all such methods are asymptotically equivalent to a non-bootstrap method known as TIC (Takeuchi (1976)), which is a generalization of AIC when the re-sampling method is non-parametric. Otherwise, for example, if the re-sampling method is parametric they are asymptotically equivalent to AIC. Therefore, the use of a bootstrap type estimate is not advantageous if enough ob- servations are available and simple calculations of a non-bootstrap estimate AIC or TIC is not a burden. At the same time, it is also true that the use of a bootstrap estimate in place of a non-bootstrap estimate is reasonable and advantageous if the non-bootstrap estimate is too complicated to evaluate analytically.},
author = {Shibata, Ritei},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shibata - 1997 - BOOTSTRAP ESTIMATE OF KULLBACK-LEIBLER INFORMATION FOR MODEL SELECTION.pdf:pdf},
journal = {Statistica Sinica},
keywords = {and phrases,bias estimation,bootstrap,information criterion,kullback-,leibler information},
pages = {375--394},
title = {{BOOTSTRAP ESTIMATE OF KULLBACK-LEIBLER}},
url = {http://www3.stat.sinica.edu.tw/statistica/oldpdf/A7n27.pdf},
volume = {7},
year = {1997}
}
@incollection{Bernardo1999,
abstract = {It is argued that hypothesis testing problems are best considered as decision problems concerning the choice of a useful probability model. Decision theory, information measures and reference analysis, are combined to propose a non-subjective Bayesian approach to nested hypothesis testing, the Bayesian Reference Criterion (BRC). The results are compared both with frequentist based procedures, and with the use of Bayes factors. The theory is illustrated with stylized examples, where alternative approaches may easily be compared.},
author = {Bernardo, M},
booktitle = {Bayesian Statistics},
file = {::},
keywords = {comparison,decision theory,logarithmic discrepancy,model,model choice,non-subjective bayesian statistics,proper scoring rules,reference analysis},
pages = {101--130},
publisher = {Oxford University Press},
title = {{Nested Hypothesis Testing : The Bayesian Reference Criterion}},
volume = {6},
year = {1999}
}
@article{Tibshirani1999,
abstract = {We propose a new criterion for model selection in prediction problems. The covariance inflation criterion adjusts the training error by the average covariance of the predictions and responses, when the prediction rule is applied to permuted versions of the data set. This criterion can be applied to general prediction problems (e.g. regression or classification) and to general prediction rules (e.g. stepwise regression, tree-based models and neural nets). As a by-product we obtain a measure of the effective number of parameters used by an adaptive procedure. We relate the covariance inflation criterion to other model selection procedures and illustrate its use in some regression and classification problems. We also revisit the conditional bootstrap approach to model selection.},
author = {Tibshirani, Robert and Knight, Keith},
doi = {10.1111/1467-9868.00191},
file = {::},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Adaptive prediction,Bootstrap,Cross-validation,Model selection,Permutation},
number = {3},
pages = {529--546},
title = {{The covariance inflation criterion for adaptive model selection}},
volume = {61},
year = {1999}
}
@inproceedings{Kearns1997,
abstract = {Sanity-check bounds were proven for the error of the leave-one-out cross-validation estimate of the generalization error. Any nontrivial bound on the error of leave-one-out relies on some notion of algorithmic stability. A weaker notion of error stability was applied to obtain sanity-check bounds for leave-one-out for other classes of learning algorithms, including training error minimization procedures and Bayesian algorithms. The necessity of error stability for good performance by the leave-one-out estimate was demonstrated by the lower bounds, and the fact that for training error minimization algorithms in the worst case bounds, still depends on the Vapnik-Chervonenkis dimension of the hypothesis class.},
author = {Kearns, Michael and Ron, Dana},
booktitle = {Proceedings of the Annual ACM Conference on Computational Learning Theory},
doi = {10.1145/267460.267491},
pages = {152--162},
title = {{Algorithmic stability and sanity-check bounds for leave-one-out cross-validation}},
year = {1997}
}
@article{Hjort2003,
abstract = {The traditional use of model selection methods in practice is to proceed as if the final selected model had been chosen in advance, without acknowledging the additional uncertainty introduced by model selection. This often means underreporting of variability and too optimistic confidence intervals. We build a general large-sample likelihood apparatus in which limiting distributions and risk properties of estimators post-selection as well as of model average estimators are precisely described, also explicitly taking modeling bias into account. This allows a drastic reduction in complexity, as competing model averaging schemes may be developed, discussed, and compared inside a statistical prototype experiment where only a few crucial quantities matter. In particular, we offer a frequentist view on Bayesian model averaging methods and give a link to generalized ridge estimators. Our work also leads to new model selection criteria. The methods are illustrated with real data applications.},
author = {Hjort, Nils Lid and Claeskens, Gerda},
doi = {10.1198/016214503000000828},
file = {::},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Bias and variance balance,Growing models,Likelihood inference,Model average estimators,Model information criteria,Moderate misspecification},
number = {464},
pages = {879--899},
title = {{Frequentist Model Average Estimators}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
volume = {98},
year = {2003}
}
@article{Ting1999,
abstract = {Stacked generalization is a general method of using a high-level model to combine lower-level models to achieve greater predictive accuracy. In this paper we address two crucial issues which have been considered to be a `black art' in classification tasks ever since the introduction of stacked generalization in 1992 by Wolpert: the type of generalizer that is suitable to derive the higher-level model, and the kind of attributes that should be used as its input. We find that best results are obtained when the higher-level model combines the confidence (and not just the predictions) of the lower-level ones. We demonstrate the effectiveness of stacked generalization for combining three different types of learning algorithms for classification tasks. We also compare the performance of stacked generalization with majority vote and published results of arcing and bagging.},
author = {Ting, Kai Ming and Witten, Ian H.},
doi = {10.1613/jair.594},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ting, Witten - 1999 - Issues in Stacked Generalization.pdf:pdf},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
month = {may},
pages = {271--289},
title = {{Issues in stacked generalization}},
url = {https://jair.org/index.php/jair/article/view/10228},
volume = {10},
year = {1999}
}
@article{Nadeau2003,
abstract = {In order to compare learning algorithms, experimental results reported in the machine learning literature often use statistical tests of significance to support the claim that a new learning algorithm generalizes better. Such tests should take into account the variability due to the choice of training set and not only that due to the test examples, as is often the case. This could lead to gross underestimation of the variance of the cross-validation estimator, and to the wrong conclusion that the new algorithm is significantly better when it is not. We perform a theoretical investigation of the variance of a variant of the cross-validation estimator of the generalization error that takes into account the variability due to the randomness of the training set as well as test examples. Our analysis shows that all the variance estimators that are based only on the results of the cross-validation experiment must be biased. This analysis allows us to propose new estimators of this variance. We show, via simulations, that tests of hypothesis about the generalization error using those new variance estimators have better properties than tests involving variance estimators currently in use and listed in Dietterich (1998). In particular, the new tests have correct size and good power. That is, the new tests do not reject the null hypothesis too often when the hypothesis is true, but they tend to frequently reject the null hypothesis when the latter is false.},
author = {Nadeau, Claude and Bengio, Yoshua},
doi = {10.1023/A:1024068626366},
file = {::},
issn = {08856125},
journal = {Machine Learning},
keywords = {Cross-validation,Generalization error,Hypothesis tests,Power,Size,Variance estimation},
number = {3},
pages = {239--281},
title = {{Inference for the generalization error}},
volume = {52},
year = {2003}
}
@article{Burman1994,
abstract = {SUMMARY: In this paper we extend the technique of cross-validation to the case where observations form a general stationary sequence. We call it h-block cross-validation, because the idea is to reduce the training set by removing the h observations preceding and following the observation in the test set. We propose taking h to be a fixed fraction of the sample size, and we add a term to our h-block cross-validated estimate to compensate for the underuse of the sample. The advantages of the proposed modification over the cross-validation technique are demonstrated via simulation. {\textcopyright} 1994 Biometrika Trust.},
author = {Burman, Prabir and Chow, Edmond and Nolan, Deborah},
doi = {10.1093/biomet/81.2.351},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/burman(1994).pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Cross-validation,Dependence,Integrated square error,Prediction error},
number = {2},
pages = {351--358},
title = {{A cross-validatory method for dependent data}},
volume = {81},
year = {1994}
}
@article{BradleyEfron1986,
abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. A regression model is fitted to an observed set of data. How accurate is the model for predicting future observations? The apparent error rate tends to underestimate the true error rate because the data have been used twice, both to fit the model and to check its accuracy. We provide simple estimates for the downward bias of the apparent error rate. The theory applies to general exponential family linear models and general measures of prediction error. Special attention is given to the case of logistic regression on binary data, with error rates measured by the proportion of misclassified cases. Several connected ideas are compared: Mallows's Cp, cross-validation, generalized cross-validation, the bootstrap, and Akaike's information criterion.},
author = {{Bradley Efron}},
doi = {10.1080/01621459.1986.10478291},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron - 1986 - How Biased is the Apparent Error Rate of a Prediction Rule.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {012,aic,bootstrap,cross-validation,generalized linear models,in this case,indicating that bias is,logistic regression,mallows,methods,not a serious problem,s c,the,the apparent error rate,the downward bias of,to be only},
number = {394},
pages = {461--470},
title = {{How Biased is the Apparent Error Rate of a Prediction Rule?}},
url = {https://www.jstor.org/stable/pdf/2289236.pdf?ab{\_}segments=0{\%}2Fdefault-2{\%}2Fcontrol{\&}refreqid=search{\%}3Ae882bb27c40795328de839432c214cd5},
volume = {81},
year = {1986}
}
@article{Elliott2007a,
abstract = {The method of multiple working hypotheses, developed by the 19th-century geologist T. C. Chamberlin, is an important philosophical contribution to the domain of hypothesis construction in science. Indeed, the concept is particularly pertinent to recent debate over the relative merits of two different statistical paradigms: null hypothesis testing and model selection. The theoretical foundations of model selection are often poorly understood by practitioners of null hypothesis testing, and even many proponents of Chamberlin's method may not fully appreciate its historical basis. We contend that the core of Chamberlin's message, communicated over a century ago, has often been forgotten or misrepresented. Therefore, we revisit his ideas in light of modern developments.The original source has great value to contemporary ecology and many related disciplines, communicating thoughtful consideration of both complexity and causality and providing hard-earned wisdom applicable to this new age of uncertainty.},
author = {Elliott, Louis P. and Brook, Barry W.},
doi = {10.1641/b570708},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/Elliott {\&} Brook 2007 - BioScience.pdf:pdf},
issn = {0006-3568},
journal = {BioScience},
keywords = {bayesian statistics,hypothesis testing,model selection,philosophy of science,statistical significance},
number = {7},
pages = {608--614},
title = {{Revisiting Chamberlin: Multiple Working Hypotheses for the 21st Century}},
volume = {57},
year = {2007}
}
@article{Shang2008a,
abstract = {Two bootstrap-corrected variants of the Akaike information criterion are proposed for the purpose of small-sample mixed model selection. These two variants are asymptotically equivalent, and provide asymptotically unbiased estimators of the expected Kullback-Leibler discrepancy between the true model and a fitted candidate model. The performance of the criteria is investigated in a simulation study where the random effects and the errors for the true model are generated from a Gaussian distribution. The parametric bootstrap is employed. The simulation results suggest that both criteria provide effective tools for choosing a mixed model with an appropriate mean and covariance structure. A theoretical asymptotic justification for the variants is presented in the Appendix. {\textcopyright} 2007 Elsevier B.V. All rights reserved.},
author = {Shang, Junfeng and Cavanaugh, Joseph E.},
doi = {10.1016/j.csda.2007.06.019},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shang, Cavanaugh - 2008 - Bootstrap variants of the Akaike information criterion for mixed model selection.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics and Data Analysis},
keywords = {AIC,Kullback-Leibler information,Model selection criteria},
month = {jan},
number = {4},
pages = {2004--2021},
publisher = {North-Holland},
title = {{Bootstrap variants of the Akaike information criterion for mixed model selection}},
url = {https://www.sciencedirect.com/science/article/pii/S0167947307002587},
volume = {52},
year = {2008}
}
@article{Richards2011,
abstract = {Behavioural ecologists often study complex systems in which multiple hypotheses could be proposed to explain observed phenomena. For some systems, simple controlled experiments can be employed to reveal part of the complexity; often, however, observational studies that incorporate a multitude of causal factors may be the only (or preferred) avenue of study. We assess the value of recently advocated approaches to inference in both contexts. Specifically, we examine the use of information theoretic (IT) model selection using Akaike's information criterion (AIC). We find that, for simple analyses, the advantages of switching to an IT-AIC approach are likely to be slight, especially given recent emphasis on biological rather than statistical significance. By contrast, the model selection approach embodied by IT approaches offers significant advantages when applied to problems of more complex causality. Model averaging is an intuitively appealing extension to model selection. However, we were unable to demonstrate consistent improvements in prediction accuracy when using model averaging with IT-AIC; our equivocal results suggest that more research is needed on its utility. We illustrate our arguments with worked examples from behavioural experiments.},
author = {Richards, Shane A. and Whittingham, Mark J. and Stephens, Philip A.},
doi = {10.1007/s00265-010-1035-8},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Richards, Whittingham, Stephens - 2011 - Model selection and model averaging in behavioural ecology the utility of the IT-AIC framework.pdf:pdf},
issn = {03405443},
journal = {Behavioral Ecology and Sociobiology},
keywords = {Effect size,Inference,Model weighting,Null hypotheses,Process-based models,Statistics},
month = {jan},
number = {1},
pages = {77--89},
publisher = {Springer-Verlag},
title = {{Model selection and model averaging in behavioural ecology: The utility of the IT-AIC framework}},
url = {http://link.springer.com/10.1007/s00265-010-1035-8},
volume = {65},
year = {2011}
}
@article{Lukacs2007,
abstract = {1. Stephens et al . (2005) argue for ‘pluralism' in statistical analysis, combining null hypothesis testing and information-theoretic (I-T) methods. We show that I-T methods are more informative even in single variable problems and we provide an ecological example. 2. I-T methods allow inferences to be made from multiple models simultaneously. We believe multimodel inference is the future of data analysis, which cannot be achieved with null hypothesis-testing approaches. 3. We argue for a stronger emphasis on critical thinking in science in general and less reliance on exploratory data analysis and data dredging. Deriving alternative hypotheses is central to science; deriving a single interesting science hypothesis and then comparing it to a default null hypothesis (e.g. ‘no difference') is not an efficient strategy for gaining knowledge. We think this single-hypothesis strategy has been relied upon too often in the past. 4. We clarify misconceptions presented by Stephens et al . (2005). 5. We think inference should be made about models, directly linked to scientific hypo- theses, and their parameters conditioned on data, Prob( H j ). | data). I-T methods provide a basis for this inference. Null hypothesis testing merely provides a probability statement about the data conditioned on a null model, Prob(data | H 0 6. Synthesis and applications . I-T methods provide a more informative approach to inference. I-T methods provide a direct measure of evidence for or against hypotheses and a means to consider simultaneously multiple hypotheses as a basis for rigorous inference. Progress in our science can be accelerated if modern methods can be used intelligently; this includes various I-T and Bayesian methods.},
author = {Lukacs, Paul M. and Thompson, William L. and Kendall, William L. and Gould, William R. and Doherty, Paul F. and Burnham, Kenneth P. and Anderson, David R.},
doi = {10.1111/j.1365-2664.2006.01267.x},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/Lukas et al 2007 - J Appl Ecol.pdf:pdf},
issn = {00218901},
journal = {Journal of Applied Ecology},
keywords = {Akaike's information criterion,Information theory,Model selection,Multimodel inference,Null hypothesis testing,Statistical analysis},
number = {2},
pages = {456--460},
title = {{Concerns regarding a call for pluralism of information theory and hypothesis testing}},
volume = {44},
year = {2007}
}
@article{Zhang2008,
abstract = {Prediction error is critical to assess model fit and evaluate model prediction. We propose the cross-validation (CV) and approximated CV methods for estimating prediction error under the Bregman divergence (BD), which embeds nearly all of the commonly used loss functions in the regression, classification procedures and machine learning literature. The approximated CV formulas are analytically derived, which facilitate fast estimation of prediction error under BD. We then study a data-driven optimal bandwidth selector for local-likelihood estimation that minimizes the overall prediction error or equivalently the covariance penalty. It is shown that the covariance penalty and CV methods converge to the same mean-prediction-error-criterion. We also propose a lower-bound scheme for computing the local logistic regression estimates and demonstrate that the algorithm monotonically enhances the target local likelihood and converges. The idea and methods are extended to the generalized varying-coefficient models and additive models. {\textcopyright} Board of the Foundation of the Scandinavian Journal of Statistics 2008.},
author = {Zhang, Chunming},
doi = {10.1111/j.1467-9469.2008.00593.x},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 2008 - Prediction error estimation under Bregman divergence for non-parametric regression and classification.pdf:pdf},
issn = {03036898},
journal = {Scandinavian Journal of Statistics},
keywords = {Cross-validation,Exponential family,Generalized varying-coefficient model,Local likelihood,Loss function,Prediction error},
month = {sep},
number = {3},
pages = {496--523},
title = {{Prediction error estimation under Bregman divergence for non-parametric regression and classification}},
volume = {35},
year = {2008}
}
@article{Kabaila2016,
abstract = {We develop an approach to evaluating frequentist model averaging procedures by considering them in a simple situation in which there are two-nested linear regression models over which we average. We introduce a general class of model averaged confidence intervals, obtain exact expressions for the coverage and the scaled expected length of the intervals, and use these to compute these quantities for the model averaged profile likelihood (MPI) and model-averaged tail area confidence intervals proposed by D. Fletcher and D. Turek. We show that the MPI confidence intervals can perform more poorly than the standard confidence interval used after model selection but ignoring the model selection process. The model-averaged tail area confidence intervals perform better than the MPI and postmodel-selection confidence intervals but, for the examples that we consider, offer little over simply using the standard confidence interval for $\theta$ under the full model, with the same nominal coverage.},
author = {Kabaila, Paul and Welsh, A. H. and Abeysekera, Waruni},
doi = {10.1111/sjos.12163},
issn = {14679469},
journal = {Scandinavian Journal of Statistics},
keywords = {Akaike information criterion,Confidence interval,Coverage probability,Expected length,Model selection,Nominal coverage,Profile likelihood,Regression models,Tail area confidence interval},
month = {mar},
number = {1},
pages = {35--48},
publisher = {Blackwell Publishing Ltd},
title = {{Model-Averaged Confidence Intervals}},
url = {http://doi.wiley.com/10.1111/sjos.12163},
volume = {43},
year = {2016}
}
@article{Kyung2010,
abstract = {Penalized regression methods for simultaneous variable selection and coefficient estimation, especially those based on the lasso of Tibshirani (1996), have received a great deal of attention in recent years, mostly through frequen-tist models. Properties such as consistency have been studied, and are achieved by different lasso variations. Here we look at a fully Bayesian formulation of the problem, which is flexible enough to encompass most versions of the lasso that have been previously considered. The advantages of the hierarchical Bayesian for-mulations are many. In addition to the usual ease-of-interpretation of hierarchical models, the Bayesian formulation produces valid standard errors (which can be problematic for the frequentist lasso), and is based on a geometrically ergodic Markov chain. We compare the performance of the Bayesian lassos to their fre-quentist counterparts using simulations, data sets that previous lasso papers have used, and a difficult modeling problem for predicting the collapse of governments around the world. In terms of prediction mean squared error, the Bayesian lasso performance is similar to and, in some cases, better than, the frequentist lasso. {\textcopyright} 2010 International Society for Bayesian Analysis.},
author = {Kyung, Minjung and Gilly, Jeff and Ghoshz, Malay and Casellax, George},
doi = {10.1214/10-BA607},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kyung et al. - 2010 - Penalized regression, standard errors, and Bayesian lassos.pdf:pdf},
issn = {19360975},
journal = {Bayesian Analysis},
keywords = {Geometric ergodicity,Gibbs Sampling,Hierarchical models,Variable selection},
number = {2},
pages = {369--412},
title = {{Penalized regression, standard errors, and Bayesian lassos}},
volume = {5},
year = {2010}
}
@article{Shao1997,
abstract = {In the problem of selecting a linear model to approximate the true unknown regression model, some necessary and/or sufficient conditions are established for the asymptotic validity of various model selection procedures such as Akaike's AIC, Mallows' C p , Shibata's FPE$\lambda$, Schwarz' BIG, generalized AIC, cross-validation, and generalized cross-validation. It is found that these selection procedures can be classified into three classes according to their asymptotic behavior. Under some fairly weak conditions, the selection procedures in one class are asymptotically valid if there exist fixed-dimension correct models; the selection procedures in another class are asymptotically valid if no fixed-dimension correct model exists. The procedures in the third class are compromises of the procedures in the first two classes. Some empirical results are also presented.},
author = {Shao, Jun},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Shao - 1997 - AN ASYMPTOTIC THEORY FOR LINEAR MODEL SELECTION.pdf:pdf},
journal = {Statistica Sinica},
keywords = {BIC,CV bias-correction,Cp,GIC,and phrases: AIC,asymptotic loss efficiency,consistency,cross-validation,squared error loss},
number = {2},
pages = {221--264},
title = {{An asymptotic theory for linear model selection}},
url = {http://www3.stat.sinica.edu.tw/statistica/oldpdf/A7n21.pdf},
volume = {7},
year = {1997}
}
@book{Kuhn2013,
abstract = {Applied Predictive Modeling covers the overall predictive modeling process, beginning with the crucial steps of data preprocessing, data splitting and foundations of model tuning. The text then provides intuitive explanations of numerous common and modern regression and classification techniques, always with an emphasis on illustrating and solving real data problems. The text illustrates all parts of the modeling process through many hands-on, real-life examples, and every chapter contains extensive R code for each step of the process. This multi-purpose text can be used as an introduction to predictive models and the overall modeling process, a practitioner's reference handbook, or as a text for advanced undergraduate or graduate level predictive modeling courses. To that end, each chapter contains problem sets to help solidify the covered concepts and uses data available in the book's R package. This text is intended for a broad audience as both an introduction to predictive models as well as a guide to applying them. Non-mathematical readers will appreciate the intuitive explanations of the techniques while an emphasis on problem-solving with real data across a wide variety of applications will aid practitioners who wish to extend their expertise. Readers should have knowledge of basic statistical ideas, such as correlation and linear regression analysis. While the text is biased against complex equations, a mathematical background is needed for advanced topics.},
author = {Kuhn, Max and Johnson, Kjell},
booktitle = {Applied Predictive Modeling},
doi = {10.1007/978-1-4614-6849-3},
isbn = {9781461468493},
issn = {0006-341X},
pages = {1--600},
title = {{Applied predictive modeling}},
year = {2013}
}
@book{James2013,
address = {New York, NY},
author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1007/978-1-4614-7138-7},
isbn = {978-1-4614-7137-0},
publisher = {Springer New York},
series = {Springer Texts in Statistics},
title = {{An Introduction to Statistical Learning}},
url = {http://link.springer.com/10.1007/978-1-4614-7138-7},
volume = {103},
year = {2013}
}
@article{Borra2010,
abstract = {The estimators most widely used to evaluate the prediction error of a non-linear regression model are examined. An extensive simulation approach allowed the comparison of the performance of these estimators for different non-parametric methods, and with varying signal-to-noise ratio and sample size. Estimators based on resampling methods such as Leave-one-out, parametric and non-parametric Bootstrap, as well as repeated Cross Validation methods and Hold-out, were considered. The methods used are Regression Trees, Projection Pursuit Regression and Neural Networks. The repeated-corrected 10-fold Cross-Validation estimator and the Parametric Bootstrap estimator obtained the best performance in the simulations. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Borra, Simone and {Di Ciaccio}, Agostino},
doi = {10.1016/j.csda.2010.03.004},
file = {::},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
keywords = {Bootstrap,Covariance penalty,Cross-validation,Extra-sample error,In-sample error,Leave-one-out,Neural networks,Optimism,Prediction error,Projection pursuit regression,Regression trees},
month = {dec},
number = {12},
pages = {2976--2989},
publisher = {Elsevier B.V.},
title = {{Measuring the prediction error. A comparison of cross-validation, bootstrap and covariance penalty methods}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947310001064},
volume = {54},
year = {2010}
}
@article{Stone1977,
abstract = {A logarithmic assessment of the performance of a predicting density is found to lead to asymptotic equivalence of choice of model by cross-validation and Akaike's criterion, when maximum likelihood estimation is used within each model.},
author = {Stone, M},
doi = {10.1111/j.2517-6161.1977.tb01603.x},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Stone - 1977 - An Asymptotic Equivalence of Choice of Model by Cross-Validation and Akaike's Criterion.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
number = {1},
pages = {44--47},
title = {{An Asymptotic Equivalence of Choice of Model by Cross-Validation and Akaike's Criterion}},
url = {https://www.jstor.org/stable/pdf/2984877.pdf?refreqid=excelsior{\%}3Aa183b28e3cb986490b21a45fcc5e3ae7},
volume = {39},
year = {1977}
}
@article{Arlot2010,
abstract = {Used to estimate the risk of an estimator or to perform model selection, cross-validation is a widespread strategy because of its simplic-ity and its (apparent) universality. Many results exist on model selection performances of cross-validation procedures. This survey intends to relate these results to the most recent advances of model selection theory, with a particular emphasis on distinguishing empirical statements from rigorous theoretical results. As a conclusion, guidelines are provided for choosing the best cross-validation procedure according to the particular features of the problem in hand.},
archivePrefix = {arXiv},
arxivId = {0907.4728},
author = {Arlot, Sylvain and Celisse, Alain},
doi = {10.1214/09-SS054},
eprint = {0907.4728},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/Arlot {\&} Celisse 2010 - Survey of CV for model selection - Statistics Surveys.pdf:pdf},
issn = {19357516},
journal = {Statistics Surveys},
keywords = {CV bias-correction,Cross-validation,Leave-one-out,Model selection},
mendeley-tags = {CV bias-correction},
pages = {40--79},
title = {{A survey of cross-validation procedures for model selection}},
url = {http://arxiv.org/abs/0907.4728{\%}0Ahttp://dx.doi.org/10.1214/09-SS054},
volume = {4},
year = {2010}
}
@article{Fushiki2011,
abstract = {Estimation of prediction accuracy is important when our aim is prediction. The training error is an easy estimate of prediction error, but it has a downward bias. On the other hand, K-fold cross-validation has an upward bias. The upward bias may be negligible in leave-one-out cross- validation, but it sometimes cannot be neglected in 5-fold or 10-fold cross-validation, which are favored from a com- putational standpoint. Since the training error has a down- ward bias and K-fold cross-validation has an upward bias, there will be an appropriate estimate in a family that con- nects the two estimates. In this paper, we investigate two families that connect the training error and K-fold cross- validation.},
author = {Fushiki, Tadayoshi},
doi = {10.1007/s11222-009-9153-8},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fushiki - 2011 - Estimation of prediction error by using K-fold cross-validation.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Bias correction,CV bias-correction,K-fold cross-validation,Large dataset,Prediction error},
mendeley-tags = {CV bias-correction},
number = {2},
pages = {137--146},
title = {{Estimation of prediction error by using K-fold cross-validation}},
url = {https://link.springer.com/content/pdf/10.1007{\%}2Fs11222-009-9153-8.pdf},
volume = {21},
year = {2011}
}

@article{Roberts2017,
author = {Roberts, David R. and Bahn, Volker and Ciuti, Simone and Boyce, Mark S. and Elith, Jane and Guillera-Arroita, Gurutzeta and Hauenstein, Severin and Lahoz-Monfort, Jos{\'{e}} J. and Schr{\"{o}}der, Boris and Thuiller, Wilfried and Warton, David I. and Wintle, Brendan A. and Hartig, Florian and Dormann, Carsten F.},
journal = {Ecography},
doi = {10.1111/ecog.02881},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Roberts{\_}et{\_}al-2017-Ecography.pdf:pdf},
issn = {16000587},
month = {aug},
number = {8},
pages = {913--929},
title = {{Cross-validation strategies for data with temporal, spatial, hierarchical, or phylogenetic structure}},
url = {http://doi.wiley.com/10.1111/ecog.02881},
volume = {40},
year = {2017}
}
@article{Hauenstein2018,
abstract = {Generalised Degrees of Freedom (GDF), as defined by Ye (1998 JASA 93:120-131), represent the sensitivity of model fits to perturbations of the data. As such they can be computed for any statistical model, making it possible, in principle, to derive the number of parameters in machine-learning approaches. Defined originally for normally distributed data only, we here investigate the potential of this approach for Bernoulli-data. GDF-values for models of simulated and real data are compared to model complexity-estimates from cross-validation. Similarly, we computed GDF-based AICc for randomForest, neural networks and boosted regression trees and demonstrated its similarity to cross-validation. GDF-estimates for binary data were unstable and inconsistently sensitive to the number of data points perturbed simultaneously, while at the same time being extremely computer-intensive in their calculation. Repeated 10-fold cross-validation was more robust, based on fewer assumptions and faster to compute. Our findings suggest that the GDF-approach does not readily transfer to Bernoulli data and a wider range of regression approaches.},
author = {Hauenstein, Severin and Wood, Simon N and Dormann, Carsten F},
doi = {10.1080/03610918.2017.1315728},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hauenstein, Wood, Dormann - 2018 - Computing AIC for black-box models using generalized degrees of freedom A comparison with cross-valid.pdf:pdf},
issn = {15324141},
journal = {Communications in Statistics: Simulation and Computation},
keywords = {Boosted regression trees,Data perturbation,Model complexity,Random forest},
number = {5},
pages = {1382--1396},
title = {{Computing AIC for black-box models using generalized degrees of freedom: A comparison with cross-validation}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=lssp20https://doi.org/./..},
volume = {47},
year = {2018}
}
@article{Liang2008,
abstract = {The conventional model selection criterion, the Akaike information criterion, aic, has been applied to choose candidate models in mixed-effects models by the consideration of marginal likelihood. Vaida {\&} Blanchard (2005) demonstrated that such a marginal aic and its small sample correction are inappropriate when the research focus is on clusters. Correspondingly, these authors suggested the use of conditional aic. Their conditional aic is derived under the assumption that the variance-covariance matrix or scaled variance-covariance matrix of random effects is known. This note provides a general conditional aic but without these strong assumptions. Simulation studies show that the proposed method is promising. {\textcopyright} 2008 Biometrika Trust.},
author = {Liang, Hua and Wu, Hulin and Zou, Guohua},
doi = {10.1093/biomet/asn023},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Liang, Wu, Zou - 2008 - A note on conditional aic for linear mixed-effects models.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {Akaike information criterion,Conditional likelihood,Longitudinal data,Marginal likelihood,Mixed-effects model,Model selection},
number = {3},
pages = {773--778},
title = {{A note on conditional aic for linear mixed-effects models}},
url = {https://www.jstor.org/stable/20441501?seq=1{\&}cid=pdf-reference{\#}references{\_}tab{\_}contents},
volume = {95},
year = {2008}
}

@book{Claeskens2008,
author = {Claeskens, Gerda and Hjort, Nils Lid},
doi = {10.1017/CBO9780511790485},
publisher = {Cambridge University Press},
series = {Cambridge Series in Statistical and Probabilistic Mathematics},
title = {{Model Selection and Model Averaging}},
year = {2008}
}
@article{Vehtari2017,
abstract = {Leave-one-out cross-validation (LOO) and the widely applicable information criterion (WAIC) are methods for estimating pointwise out-of-sample prediction accuracy from a fitted Bayesian model using the log-likelihood evaluated at the posterior simulations of the parameter values. LOO and WAIC have various advantages over simpler estimates of predictive error such as AIC and DIC but are less used in practice because they involve additional computational steps. Here we lay out fast and stable computations for LOO and WAIC that can be performed using existing simulation draws. We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights. Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in the finite case with weak priors or influential observations. As a byproduct of our calculations, we also obtain approximate standard errors for estimated predictive errors and for comparison of predictive errors between two models. We implement the computations in an R package called loo and demonstrate using models fit with the Bayesian inference package Stan.},
archivePrefix = {arXiv},
arxivId = {1507.04544},
author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
doi = {10.1007/s11222-016-9696-4},
eprint = {1507.04544},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vehtari, Gelman, Gabry - 2017 - Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {Bayesian computation,K-fold cross-validation,Leave-one-out cross-validation (LOO),Pareto smoothed importance sampling (PSIS),Stan,Widely applicable information criterion (WAIC)},
number = {5},
pages = {1413--1432},
publisher = {Springer New York LLC},
title = {{Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC}},
volume = {27},
year = {2017}
}
@article{Vidoni2015,
abstract = {{\textcopyright} 2015 John Wiley {\&} Sons, Ltd. This paper concerns a class of model selection criteria based on cross-validation techniques and estimative predictive densities. Both the simple or leave-one-out and the multifold or leave-m-out cross-validation procedures are considered. These cross-validation criteria define suitable estimators for the expected Kullback-Liebler risk, which measures the expected discrepancy between the fitted candidate model and the true one. In particular, we shall investigate the potential bias of these estimators, under alternative asymptotic regimes for m. The results are obtained within the general context of independent, but not necessarily identically distributed, observations and by assuming that the candidate model may not contain the true distribution. An application to the class of normal regression models is also presented, and simulation results are obtained in order to gain some further understanding on the behavior of the estimators.},
author = {Vidoni, Paolo},
doi = {10.1111/stan.12070},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Vidoni - 2015 - Estimating the Kullback-Liebler risk based on multifold cross-validation.pdf:pdf},
issn = {14679574},
journal = {Statistica Neerlandica},
keywords = {AIC,Cross-validation,Kullback-Liebler information,Likelihood,Misspecification,Predictive density},
month = {nov},
number = {4},
pages = {510--540},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Estimating the Kullback-Liebler risk based on multifold cross-validation}},
url = {http://doi.wiley.com/10.1111/stan.12070},
volume = {69},
year = {2015}
}
@article{Johnson1996,
abstract = {Percentage of body fat, age, weight, height, and ten body circumference measurements (e.g., abdomen) are recorded for 252 men. Body fat, one measure of health, has been accurately estimated by an underwater weighing technique. Fitting body fat to the other measurements using multiple regression provides a convenient way of estimating body fat for men using only a scale and a measuring tape. This dataset can be used to show students the utility of multiple regression and to provide practice in model building.},
author = {Johnson, Roger W.},
doi = {10.1080/10691898.1996.11910505},
file = {::},
issn = {1069-1898},
journal = {Journal of Statistics Education},
keywords = {Multiple regression},
month = {mar},
number = {1},
publisher = {Informa UK Limited},
title = {{Fitting Percentage of Body Fat to Simple Body Measurements}},
url = {https://www.tandfonline.com/doi/abs/10.1080/10691898.1996.11910505},
volume = {4},
year = {1996}
}
@article{Charkhi2018,
abstract = {Ignoring the model selection step in inference after selection is harmful. In this paper we study the asymptotic distribution of estimators after model selection using the Akaike information criterion. First, we consider the classical setting in which a true model exists and is included in the candidate set of models. We exploit the overselection property of this criterion in constructing a selection region, and we obtain the asymptotic distribution of estimators and linear combinations thereof conditional on the selected model. The limiting distribution depends on the set of competitive models and on the smallest overparameterized model. Second, we relax the assumption on the existence of a true model and obtain uniform asymptotic results. We use simulation to study the resulting post-selection distributions and to calculate confidence regions for the model parameters, and we also apply the method to a diabetes dataset.},
author = {Charkhi, Ali and Claeskens, Gerda},
doi = {10.1093/biomet/asy018},
file = {::},
issn = {14643510},
journal = {Biometrika},
keywords = {Akaike information criterion,Confidence region,Likelihood model,Model selection,Post-selection inference},
number = {3},
pages = {645--664},
title = {{Asymptotic post-selection inference for the Akaike information criterion}},
volume = {105},
year = {2018}
}
@article{Piironen2017,
abstract = {The goal of this paper is to compare several widely used Bayesian model selection methods in practical model selection problems, highlight their differences and give recommendations about the preferred approaches. We focus on the variable subset selection for regression and classification and perform several numerical experiments using both simulated and real world data. The results show that the optimization of a utility estimate such as the cross-validation (CV) score is liable to finding overfitted models due to relatively high variance in the utility estimates when the data is scarce. This can also lead to substantial selection induced bias and optimism in the performance evaluation for the selected model. From a predictive viewpoint, best results are obtained by accounting for model uncertainty by forming the full encompassing model, such as the Bayesian model averaging solution over the candidate models. If the encompassing model is too complex, it can be robustly simplified by the projection method, in which the information of the full model is projected onto the submodels. This approach is substantially less prone to overfitting than selection based on CV-score. Overall, the projection method appears to outperform also the maximum a posteriori model and the selection of the most probable variables. The study also demonstrates that the model selection can greatly benefit from using cross-validation outside the searching process both for guiding the model size selection and assessing the predictive performance of the finally selected model.},
archivePrefix = {arXiv},
arxivId = {1503.08650},
author = {Piironen, Juho and Vehtari, Aki},
doi = {10.1007/s11222-016-9649-y},
eprint = {1503.08650},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Piironen, Vehtari - 2017 - Comparison of Bayesian predictive methods for model selection.pdf:pdf},
issn = {0960-3174},
journal = {Statistics and Computing},
keywords = {Bayesian model selection,Cross-validation,Projection,Reference model,Selection bias},
month = {may},
number = {3},
pages = {711--735},
publisher = {Springer New York LLC},
title = {{Comparison of Bayesian predictive methods for model selection}},
url = {http://link.springer.com/10.1007/s11222-016-9649-y},
volume = {27},
year = {2017}
}
@article{Fish2014,
author = {Fish, Colorado Cooperative and Collins, Fort},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/Burnham {\&} Anderson 2014 - Ecology.pdf:pdf},
journal = {Ecology},
number = {March},
pages = {627--630},
title = {{P values are only an index to evidence : 20th- vs . 21st-century statistical science}},
volume = {95},
year = {2014}
}
@article{Shen2004,
abstract = {Typical modeling strategies involve model selection, which has a significant effect on inference of estimated parameters. Common practice is to use a selected model ignoring uncertainty introduced by the process of model selection. This could yield overoptimistic inferences, resulting in false discovery. In this article we develop a general methodology via optimal approximation for estimating the mean and variance of complex statistics that involve the process of model selection. This allows us to make approximately unbiased inferences, taking into account the selection process. We examine the operating characteristics of the proposed methodology via asymptotic analyses and simulations. These results show that the proposed methodology yields correct inferences and outperforms common alternatives.},
archivePrefix = {arXiv},
arxivId = {1410.2597},
author = {Shen, Xiaotong and Huang, Hsin Cheng and Ye, Jimmy},
doi = {10.1198/016214504000001097},
eprint = {1410.2597},
file = {::},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Bootstrap,Nonparametric,Parametric,Variable selection,Wavelet thresholding},
number = {467},
pages = {751--762},
title = {{Inference after model selection}},
volume = {99},
year = {2004}
}
@article{Zhang1993,
abstract = {A natural extension of the simple leave-one-out cross validation (CV) method is to allow the deletion of more than one observations. In this article, several notions of the multifold cross validation (MCV) method have been discussed. In the context of variable selection under a linear regres- sion model, we show that the delete-d MCV criterion is asymptotically equivalent to the well known FPE criterion. Two computationally more feasible methods, the r-fold cross validation and the repeated learning-test- ing criterion, are also studied. The performance of these criteria are compared with the simple leave-one-out cross validation method. Simula- tion results are obtained to gain some understanding on the small sample properties of these methods.},
author = {Zhang, Ping},
doi = {10.1214/aos/1176349027},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Zhang - 1993 - Model Selection Via Multifold Cross Validation.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
number = {1},
pages = {299--313},
title = {{Model Selection Via Multifold Cross Validation}},
url = {https://www.jstor.org/stable/pdf/3035592.pdf?ab{\_}segments=0{\%}2Fdefault-2{\%}2Fcontrol{\&}refreqid=search{\%}3A4508025ccbb355c23fcd92dd4eabf118},
volume = {21},
year = {1993}
}
@incollection{Akaike1973,
address = {Academiai Kiado, Budapest},
author = {Akaike, H},
booktitle = {Second International Symposium on Information Theory (Tsahkadsor, 1971)},
editor = {{B. N. Petrov and F. Csaki}},
pages = {267--281},
title = {{Information theory and an extension of the maximum likelihood principle}},
year = {1973}
}
@article{Hastie2012a,
abstract = {We review the across-model simulation approach to computation for Bayesian model determination, based on the reversible jump Markov chain Monte Carlo method. Advantages, difficulties and variations of the methods are discussed. We also discuss some limitations of the ideal Bayesian view of the model determination problem, for which no computational methods can provide a cure. {\textcopyright} 2012 The Authors. Statistica Neerlandica {\textcopyright} 2012 VVS.},
author = {Hastie, David I. and Green, Peter J.},
doi = {10.1111/j.1467-9574.2012.00516.x},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Hastie(2012).pdf:pdf},
issn = {00390402},
journal = {Statistica Neerlandica},
keywords = {Across-model sampling,Bayes factors,Bayesian model determination,Posterior model probabilities,Transdimensional inference,Variable dimension problems},
month = {aug},
number = {3},
pages = {309--338},
title = {{Model choice using reversible jump Markov chain Monte Carlo}},
url = {http://doi.wiley.com/10.1111/j.1467-9574.2012.00516.x},
volume = {66},
year = {2012}
}
@article{Yang1999,
abstract = {Model combining (mixing) provides an alternative to model selection. An algorithm ARM was recently proposed by the author to combine different regression models/methods. In this work, an improved risk bound for ARM is obtained. In addition to some theoretical observations on the issue of selection versus combining, simulations are conducted in the context of linear regression to compare performance of ARM with the familiar model selection criteria AIC and BIC, and also with some Bayesian model averaging (BMA) methods. The simulation suggests the following. Selection can yield a smaller risk when the random error is weak relative to the signal. However, when the random noise level gets higher, ARM produces a better or even much better estimator. That is, mixing appropriately is advantageous when there is a certain degree of uncertainty in choosing the best model. In addition, it is demonstrated that when AIC and BIC are combined, the mixed estimator automatically behaves like the better one. A comparison with bagging (Breiman (1996)) suggests that ARM does better than simply stabilizing model selection estimators. In our simulation, ARM also performs better than BMA techniques based on BIC approximation.},
author = {Yang, Yuhong},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang - 1999 - Statistics Preprints Regression with multiple candidate models Selecting or mixing.pdf:pdf},
issn = {10170405},
journal = {Statistica Sinica},
keywords = {ARM,Combining procedures,Model averaging,Model selection},
number = {3},
pages = {783--809},
title = {{Regression with multiple candidate models: Selecting or mixing?}},
url = {http://lib.dr.iastate.edu/stat{\_}las{\_}preprints},
volume = {13},
year = {2003}
}
@article{Spiegelhalter2002,
abstract = {We consider the problem of comparing complex hierarchical models in which the number of parameters is not clearly defined. Using an information theoretic argument we derive a measure pD for the effective number of parameters in a model as the difference between the posterior mean of the deviance and the deviance at the posterior means of the parameters of interest. In general pD approximately corresponds to the trace of the product of Fisher's information and the posterior covariance, which in normal models is the trace of the 'hat' matrix projecting observations onto fitted values. Its properties in exponential families are explored. The posterior mean deviance is suggested as a Bayesian measure of fit or adequacy, and the contributions of individual observations to the fit and complexity can give rise to a diagnostic plot of deviance residuals against leverages. Adding pD to the posterior mean deviance gives a deviance information criterion for comparing models, which is related to other information criteria and has an approximate decision theoretic justification. The procedure is illustrated in some examples, and comparisons are drawn with alternative Bayesian and classical proposals. Throughout it is emphasized that the quantities required are trivial to compute in a Markov chain Monte Carlo analysis.},
author = {Spiegelhalter, David J. and Best, Nicola G. and Carlin, Bradley P. and {Van Der Linde}, Angelika},
doi = {10.1111/1467-9868.00353},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Spiegelhalter2002.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
keywords = {Bayesian model comparison,Decision theory,Deviance information criterion,Effective number of parameters,Hierarchical models,Information theory,Leverage,Markov chain Monte Carlo methods,Model dimension},
month = {oct},
number = {4},
pages = {583--616},
title = {{Bayesian measures of model complexity and fit}},
url = {http://doi.wiley.com/10.1111/1467-9868.00353},
volume = {64},
year = {2002}
}
@article{MacNally2018,
abstract = {{\textcopyright} 2017 British Ecological Society. Information criteria (ICs) are used widely for data summary and model building in ecology, especially in applied ecology and wildlife management. Although ICs are useful for distinguishing among rival candidate models, ICs do not necessarily indicate whether the "best" model (or a model-averaged version) is a good representation of the data or whether the model has useful "explanatory" or "predictive" ability. As editors and reviewers, we have seen many submissions that did not evaluate whether the nominal "best" model(s) found using IC is a useful model in the above sense. We scrutinized six leading ecological journals for papers that used IC to compare models. More than half of papers using IC for model comparison did not evaluate the adequacy of the best model(s) in either "explaining" or "predicting" the data. Synthesis and applications. Authors need to evaluate the adequacy of the model identified as the "best" model by using information criteria methods to provide convincing evidence to readers and users that inferences from the best models are useful and reliable.},
author = {{Mac Nally}, Ralph and Duncan, Richard P. and Thomson, James R. and Yen, Jian D.L.},
doi = {10.1111/1365-2664.13060},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/Nally{\_}et{\_}al-2018-Journal{\_}of{\_}Applied{\_}Ecology.pdf:pdf},
issn = {13652664},
journal = {Journal of Applied Ecology},
keywords = {cross-validation,ecological models,external validation,goodness-of-fit,internal validation,model adequacy,model averaging,model selection},
number = {3},
pages = {1441--1444},
title = {{Model selection using information criteria, but is the “best” model any good?}},
volume = {55},
year = {2018}
}
@incollection{Akaike1998,
abstract = {In this paper it is shown that the classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion. This observation shows an extension of the principle to provide answers to many practical problems of statistical model fitting.},
author = {Akaike, Hirotogu},
doi = {10.1007/978-1-4612-1694-0_15},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/akaike1998.pdf:pdf},
pages = {199--213},
publisher = {Springer, New York, NY},
title = {{Information Theory and an Extension of the Maximum Likelihood Principle}},
url = {http://link.springer.com/10.1007/978-1-4612-1694-0{\_}15},
year = {1998}
}
@article{Richards2005,
author = {Richards, Shane A.},
journal = {Ecology},
doi = {10.1890/05-0074},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Richards - 2005 - TESTING ECOLOGICAL THEORY USING THE INFORMATION-THEORETIC APPROACH EXAMPLES AND CAUTIONARY RESULTS.pdf:pdf},
issn = {00129658},
keywords = {Akaike Information Criterion (AIC),Foraging,Model averaging,Model selection,Theoretical ecology},
month = {oct},
number = {10},
pages = {2805--2814},
publisher = {John Wiley {\&} Sons, Ltd},
title = {{Testing ecological theory using the information-theoretic approach: Examples and cautionary results}},
url = {http://doi.wiley.com/10.1890/05-0074},
volume = {86},
year = {2005}
}


@article{Burnham2015,
author = {Burnham, Kenneth P.},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/AICRelativeVariableImportanceWeights-Burnham.pdf:pdf},
title = {{Multimodel Inference : Understanding AIC relative variable importance values Kenneth P . Burnham Colorado State University Fort Collins , Colorado 80523 Abstract}},
year = {2015}
}
@article{Leeb2006,
abstract = {We consider the problem of estimating the conditional distribution of a post-model-selection estimator where the conditioning is on the selected model. The notion of a post-model-selection estimator here refers to the combined procedure resulting from first selecting a model (e.g., by a model selection criterion such as AIC or by a hypothesis testing procedure) and then estimating the parameters in the selected model (e.g., by least-squares or maximum likelihood), all based on the same data set. We show that it is impossible to estimate this distribution with reasonable accuracy even asymptotically. In particular, we show that no estimator for this distribution can be uniformly consistent (not even locally). This follows as a corollary to (local) minimax lower bounds on the performance of estimators for this distribution. Similar impossibility results are also obtained for the conditional distribution of linear functions (e.g., predictors) of the post-model-selection estimator. {\textcopyright} Institute of Mathematical Statistics, 2006.},
author = {Leeb, Hannes and P{\"{o}}scher, Benedikt M.},
doi = {10.1214/009053606000000821},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Akaike's information criterion AIC,Consistency,Inference after model selection,Lower risk bound,Model uncertainty,Post-model-selection estimator,Pre-test estimator,Selection of regressors,Thresholding,Uniform consistency},
month = {oct},
number = {5},
pages = {2554--2591},
title = {{Can one estimate the conditional distribution of post-model-selection estimators?}},
volume = {34},
year = {2006}
}
@book{Breiman1984,
abstract = {The methodology used to construct tree structured rules is the focus of this monograph. Unlike many other statistical procedures, which moved from pencil and paper to calculators, this text's use of trees was unthinkable before computers. Both the practical and theoretical sides have been developed in the authors' study of tree methods. Classification and Regression Trees reflects these two sides, covering the use of trees as a data analysis method, and in a more mathematical framework, proving some of their fundamental properties.},
author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
booktitle = {Classification and Regression Trees},
isbn = {0-534-98053-8},
month = {jan},
pages = {1--358},
publisher = {Wadsworth {\&} Brooks/Cole Advanced Books {\&} Software,},
title = {{Classification and regression trees}},
year = {1984}
}


@book{Bolker2008,
author = {Bolker, Benjamin M.},
doi = {10.1111/j.1442-9993.2010.02210.x},
isbn = {9781400840908},
publisher = {Princeton University Press},
title = {{Ecological models and data in R}},
year = {2008}
}


@article{Banner2017,
abstract = {Model choice is usually an inevitable source of uncertainty in model-based statistical analyses. While the focus of model choice was traditionally on methods for choosing a single model, methods to formally account for multiple models within a single analysis are now accessible to many researchers. The specific technique of model averaging was developed to improve predictive ability by combining predictions from a set of models. However, it is now often used to average regression coefficients across multiple models with the ultimate goal of capturing a variable's overall effect. This use of model averaging implicitly assumes the same parameter exists across models so that averaging is sensible. While this assumption may initially seem tenable, regression coefficients associated with particular explanatory variables may not hold equivalent interpretations across all of the models in which they appear, making explanatory inference about covariates challenging. Accessibility to easily implementable software, concerns about being criticized for ignoring model uncertainty, and the chance to avoid having to justify choice of a final model have all led to the increasing popularity of model averaging in practice. We see a gap between the theoretical development of model averaging and its current use in practice, potentially leaving well-intentioned researchers with unclear inferences or difficulties justifying reasons for using (or not using) model averaging. We attempt to narrow this gap by revisiting some relevant foundations of regression modeling, suggesting more explicit notation and graphical tools, and discussing how individual model results are combined to obtain a model averaged result. Our goal is to help researchers make informed decisions about model averaging and to encourage question-focused modeling over method-focused modeling.},
author = {Banner, Katharine M. and Higgs, Megan D.},
doi = {10.1002/eap.1419},
issn = {19395582},
journal = {Ecological Applications},
keywords = {Bayesian model averaging,explanatory inference,linear regression,model averaging,model selection,multimodel inference,predictive inference},
month = {jan},
number = {1},
pages = {78--93},
pmid = {27874997},
publisher = {Ecological Society of America},
title = {{Considerations for assessing model averaging of regression coefficients:}},
url = {http://doi.wiley.com/10.1002/eap.1419},
volume = {27},
year = {2017}
}
@misc{Claeskens2003,
abstract = {A variety of model selection criteria have been developed, of general and specific types. Most of these aim at selecting a single model with good overall properties, for example, formulated via average prediction quality or shortest estimated overall distance to the true model. The Akaike, the Bayesian, and the deviance information criteria, along with many suitable variations, are examples of such methods. These methods are not concerned, however, with the actual use of the selected model, which varies with context and application. The present article takes the view that the model selector should instead focus on the parameter singled out for interest; in particular, a model that gives good precision for one estimand may be worse when used for inference for another estimand. We develop a method that, for a given focus parameter, estimates the precision of any submodel-based estimator. The framework is that of large-sample likelihood inference. Using an unbiased estimate of limiting risk, we propose a focused information criterion for model selection. We investigate and discuss properties of the method, establish some connections to Akaike's information criterion, and illustrate its use in a variety of situations.},
author = {Claeskens, Gerda and Hjort, Nils Lid},
booktitle = {Journal of the American Statistical Association},
doi = {10.1198/016214503000000819},
file = {::},
issn = {01621459},
keywords = {Akaike's information criterion,Bias and variance balance,Focused information criterion,Logistic regression,Moderate misspecification,Variable selection},
month = {dec},
number = {464},
pages = {900--916},
publisher = {Taylor {\&} Francis},
title = {{The Focused Information Criterion}},
volume = {98},
year = {2003}
}

@book{Burnham2002,
address = {New York},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Burnham, Kenneth P. and Anderson, David R.},
booktitle = {Book},
edition = {Second},
eprint = {arXiv:1011.1669v3},
isbn = {0-387-95364-7},
issn = {03043800},
pages = {515},
pmid = {48557578},
publisher = {Springer-Verlag},
title = {{Model selection and multimodel inference: a practical information-theoretic approach}},
year = {2002}
}


@incollection{Cavanaugh2008,
address = {Boston, MA},
author = {Cavanaugh, Joseph E. and Davies, Simon L. and Neath, Andrew A.},
booktitle = {Statistical Models and Methods for Biomedical and Technical Systems},
doi = {10.1007/978-0-8176-4619-6_33},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Cavanaugh.pdf:pdf},
pages = {473--486},
publisher = {Birkh{\"{a}}user Boston},
title = {{Discrepancy-Based Model Selection Criteria Using Cross-Validation}},
url = {http://link.springer.com/10.1007/978-0-8176-4619-6{\_}33},
year = {2008}
}
@article{HORNE2006,
abstract = {Fixed kernel density analysis with least squares cross-validation (LSCVh) choice of the smoothing parameter is currently recommended for home-range estimation. However, LSCVh has several drawbacks, including high variability, a tendency to undersmooth data, and multiple local minima in the LSCVh function. An alternative to LSCVh is likelihood cross-validation (CVh). We used computer simulations to compare estimated home ranges using fixed kernel density with CVh and LSCVh to true underlying distributions. Likelihood cross-validation generally performed better than LSCVh, producing estimates with better fit and less variability and it was especially beneficial at sample sizes {\textless}similar to 50. Because CVh is based on minimizing the Kullback-Leibler distance and LSCVh the integrated squared error, for each of these measures of discrepancy, we discussed their foundation and general use, statistical properties as they relate to home-range analysis, and the biological or practical interpretation of these statistical properties. We found 2 important problems related to computation of kernel home-range estimates, including multiple minima in the LSCVh and CVh functions and discrepancies among estimates from current home-range software. Choosing an appropriate smoothing parameter is critical when using kernel methods to estimate animal home ranges, and our study provides useful guidelines when making this decision.},
author = {HORNE, JON S. and GARTON, EDWARD O.},
doi = {10.2193/0022-541x(2006)70[641:lcvlsc]2.0.co;2},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/HORNE, GARTON - 2006 - Likelihood Cross-Validation Versus Least Squares Cross-Validation for Choosing the Smoothing Parameter in Kernel.pdf:pdf},
issn = {0022-541X},
journal = {Journal of Wildlife Management},
keywords = {Kullback-Leibler distance,home range,kernel methods,least squares cross-validation,likelihood cross-validation,smoothing parameter,utilization distribution},
month = {jun},
number = {3},
pages = {641--648},
publisher = {Wildlife Society},
title = {{Likelihood Cross-Validation Versus Least Squares Cross-Validation for Choosing the Smoothing Parameter in Kernel Home-Range Analysis}},
url = {https://bioone.org/journals/Journal-of-Wildlife-Management/volume-70/issue-3/0022-541X(2006)70[641:LCVLSC]2.0.CO;2/Likelihood-Cross-Validation-Versus-Least-Squares-Cross-Validation-for-Choosing/10.2193/0022-541X(2006)70[641:LCVLSC]2.0.CO;2.full},
volume = {70},
year = {2006}
}


@book{Hastie2009old,
abstract = {* A more theoretical book on the same subject as the book on statistical learning by Hastie/Tibshirani/Friedman},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {The Elements of Statistical Learning},
doi = {10.1007/b94608},
eprint = {arXiv:1011.1669v3},
isbn = {9780387848570},
issn = {03436993},
number = {2},
pages = {83--85},
pmid = {15512507},
publisher = {Springer Series in Statistics},
title = {{Springer Series in Statistics}},
url = {http://www.springerlink.com/index/D7X7KX6772HQ2135.pdf},
volume = {27},
year = {2009}
}


@article{Burman1961,
abstract = {New methods for the estimation of optimal transformations of random variables are proposed here. These methods have a much wider applicability. The attractiveness of these methods is that they are computationally a lot less expensive than ordinary cross validation.},
author = {Burman, Prabir},
doi = {10.2307/25050800},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burman - 1961 - Estimation of Optimal Transformations Using v-Fold Cross Validation and Repeated Learning-Testing Methods.pdf:pdf},
issn = {0581572X},
journal = {The Indian Journal of Statistics, Series A (1961-2002)},
number = {3},
pages = {314--345},
title = {{Estimation of Optimal Transformations Using v-Fold Cross Validation and Repeated Learning-Testing Methods}},
url = {https://about.jstor.org/terms},
volume = {52},
year = {1990}
}
@article{Ando2010,
abstract = {This paper investigates the performance of the predictive distributions of Bayesian models. To overcome the difficulty of evaluating the predictive likelihood, we introduce the concept of expected log-predictive likelihoods for Bayesian models, and propose an estimator of the expected log-predictive likelihood. The estimator is derived by correcting the asymptotic bias of the log-likelihood of the predictive distribution as an estimate of its expected value. We investigate the relationship between the proposed criterion and the traditional information criteria and show that the proposed criterion is a natural extension of the traditional ones. A new model selection criterion and a new model averaging method are then developed, with the weights for the individual models being dependent on their expected log-predictive likelihoods. We examine the performance of the proposed method using Monte Carlo experiments and a real example, which concerns the prediction of quarterly growth rates of real gross domestic product in the G7 countries. Out-of-sample forecasts show that the proposed methodology outperforms other methods available in the literature. {\textcopyright} 2009 International Institute of Forecasters.},
author = {Ando, Tomohiro and Tsay, Ruey},
doi = {10.1016/j.ijforecast.2009.08.001},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ando, Tsay - 2010 - Predictive likelihood for Bayesian model selection and averaging.pdf:pdf},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Finance,Model averaging,Model selection,Predictive likelihood},
month = {oct},
number = {4},
pages = {744--763},
title = {{Predictive likelihood for Bayesian model selection and averaging}},
volume = {26},
year = {2010}
}
@article{Bengio2004,
abstract = {Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms (in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don't take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance. This is confirmed by numerical experiments in which the three components of the variance are compared when the difficulty of the learning problem and the number of folds are varied.},
author = {Bengio, Yoshua and Grandvalet, Yves},
file = {::},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Cross-validation,K-fold cross-validation,Statistical comparisons of algorithms,Variance estimators},
pages = {1089--1105},
title = {{No unbiased estimator of the variance of K-fold cross-validation}},
volume = {5},
year = {2004}
}
@article{Lee2016,
abstract = {We develop a general approach to valid inference after model selection. At the core of our framework is a result that characterizes the distribution of a post-selection estimator conditioned on the selection event.We specialize the approach to model selection by the lasso to form valid confidence intervals for the selected coefficients and test whether all relevant variables have been included in the model.},
archivePrefix = {arXiv},
arxivId = {1311.6238},
author = {Lee, Jason D and Sun, Dennis L. and Sun, Yuekai and Taylor, Jonathan E},
doi = {10.1214/15-AOS1371},
eprint = {1311.6238},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Confidence interval,Hypothesis test,Lasso,Model selection},
number = {3},
pages = {907--927},
title = {{Exact post-selection inference, with application to the lasso}},
volume = {44},
year = {2016}
}
@article{Broms2016,
abstract = {While multi-species occupancy models (MSOMs) are emerging as a popular method for analyzing biodiversity data, formal checking and validation approaches for this class of models have lagged behind. Concurrent with the rise in application of MSOMs among ecologists, a quiet regime shift is occurring in Bayesian statistics where predictive model comparison approaches are experiencing a resurgence. Unlike single-species occupancy models that use integrated likelihoods, MSOMs are usually couched in a Bayesian framework and contain multiple levels. Standard model checking and selection methods are often unreliable in this setting and there is only limited guidance in the ecological literature for this class of models. We examined several different contemporary Bayesian hierarchical approaches for checking and validating MSOMs and applied these methods to a freshwater aquatic study system in Colorado, USA, to better understand the diversity and distributions of plains fishes. Our findings indicated distinct differences among model selection approaches, with cross-validation techniques performing the best in terms of prediction.},
author = {Broms, Kristin M. and Hooten, Mevin B. and Fitzpatrick, Ryan M.},
doi = {10.1890/15-1471.1},
issn = {00129658},
journal = {Ecology},
keywords = {Bayesian hierarchical models,Biodiversity,Cross-validation,Plains fish,South platte river basin,Species distribution maps},
pmid = {27859174},
title = {{Model selection and assessment for multi-species occupancy models}},
year = {2016}
}
@Book{southwood1978,
author = {Southwood, T R E},
doi = {10.1007/978-94-009-1225-0},
edition = {Second},
publisher = {Springer Netherlands},
title = {{Ecological methods with particular reference to the study of insect populations}},
year = {1978}
}
@article{YANAGIHARA2012,
abstract = {The cross-validation (CV) criterion is known to be asecond-order unbiased estimator of the risk function measuring the discrepancy between the candidate model and the true model, as well as the generalized information criterion (GIC) and the extended information criterion (EIC).In the present article, we show that the 2kth-order unbiased estimator can be obtained using a linear combination from the leave-one-out CV criterion to the leave-k-out CV criterion. The proposed scheme is unique in that a bias smaller than that of a jackknife method can be obtained without any analytic calculation, that is, it is not necessary to obtain the explicit form of several terms in an asymptotic expansion of the bias. Furthermore, the proposed criterion can be regarded as a finite correction of a bias-corrected CV criterion by using scalar coefficients in a bias-corrected EIC obtained by the bootstrap iteration.},
author = {Yanagihara, Hirokazu and Fujisawa, Hironori},
doi = {10.1111/j.1467-9469.2011.00754.x},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/YANAGIHARA, FUJISAWA - 2012 - Iterative Bias Correction of the Cross-Validation Criterion.pdf:pdf},
issn = {03036898},
journal = {Scandinavian Journal of Statistics},
keywords = {Asymptotic expansion,Bias correction,Bootstrap iteration,Cross-validation criterion,EIC,GIC,Leave-k-out cross-validation,Model selection},
month = {mar},
number = {1},
pages = {116--130},
publisher = {John Wiley {\&} Sons, Ltd (10.1111)},
title = {{Iterative Bias Correction of the Cross-Validation Criterion}},
url = {http://doi.wiley.com/10.1111/j.1467-9469.2011.00754.x},
volume = {39},
year = {2012}
}
@article{Yang2005,
author = {Yang, Yuhong},
doi = {10.1093/biomet/92.4.937},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang - 2005 - Can the strengths of AIC and BIC be shared A conflict between model indentification and regression estimation.pdf:pdf},
issn = {1464-3510},
journal = {Biometrika},
month = {dec},
number = {4},
pages = {937--950},
publisher = {Narnia},
title = {{Can the strengths of AIC and BIC be shared? A conflict between model indentification and regression estimation}},
url = {http://academic.oup.com/biomet/article/92/4/937/389439/Can-the-strengths-of-AIC-and-BIC-be-shared-A},
volume = {92},
year = {2005}
}
@article{Gelman2014,
abstract = {We review the Akaike, deviance, and Watanabe-Akaike information criteria from a Bayesian perspective, where the goal is to estimate expected out-of-sample-prediction error using a bias-corrected adjustment of within-sample error. We focus on the choices involved in setting up these measures, and we compare them in three simple examples, one theoretical and two applied. The contribution of this paper is to put all these information criteria into a Bayesian predictive context and to better understand, through small examples, how these methods can apply in practice.},
archivePrefix = {arXiv},
arxivId = {1307.5928},
author = {Gelman, Andrew and Hwang, Jessica and Vehtari, Aki},
doi = {10.1007/s11222-013-9416-2},
eprint = {1307.5928},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman, Hwang, Vehtari - 2014 - Understanding predictive information criteria for Bayesian models.pdf:pdf},
issn = {15731375},
journal = {Statistics and Computing},
keywords = {AIC,Bayes,Cross-validation,DIC,Prediction,WAIC},
month = {nov},
number = {6},
pages = {997--1016},
publisher = {Springer New York LLC},
title = {{Understanding predictive information criteria for Bayesian models}},
volume = {24},
year = {2014}
}
@article{Hu2008,
abstract = {The problem of model selection in generalized linear models amounts to selecting a subset of useful covariates from a set of possible covariates and choosing a link function from a set of possible link functions. A model selection procedure based on a modified R2 statistic is proposed. Like in linear models, R2 statistics in generalized linear models are used to quantify the proportion of variance in the response explained by covariates. Model selection using R2 statistics is natural for investigators who are familiar with the use of R2 statistics. The modified R2 statistic is obtained by introducing an extra penalty term on the complexity of the candidate model. Under weak conditions, the proposed procedure is shown to be consistent in the sense that with probability tending to one (as the sample size increases) the selected model equals the optimal model between the response and covariates. Simulation results are presented to demonstrate the effectiveness of the proposed procedure in finite sample applications. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Hu, Bo and Shao, Jun},
doi = {10.1016/j.jspi.2007.12.009},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hu, Shao - 2008 - Generalized linear model selection using R2.pdf:pdf},
issn = {03783758},
journal = {Journal of Statistical Planning and Inference},
keywords = {Consistency,Generalized linear model,Model selection,Optimal model,R2},
month = {dec},
number = {12},
pages = {3705--3712},
publisher = {North-Holland},
title = {{Generalized linear model selection using R2}},
url = {https://www.sciencedirect.com/science/article/pii/S0378375808001250?via{\%}3Dihub},
volume = {138},
year = {2008}
}
@article{Watanabe2010,
abstract = {In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2$\lambda$/n, where $\lambda$ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion. {\textcopyright} 2010 Sumio Watanabe.},
archivePrefix = {arXiv},
arxivId = {1004.2316},
author = {Watanabe, Sumio},
eprint = {1004.2316},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {Birational invariant,Cross-validation,Information criterion,Singular learning machine},
pages = {3571--3594},
title = {{Asymptotic equivalence of Bayes cross validation and widely applicable information criterion in singular learning theory}},
volume = {11},
year = {2010}
}
@article{Efron1997,
abstract = {A study investigates the error rate of a rule for predicting future responses constructed from a training set of data. Results are nonparametric and apply to any possible prediction rule.},
author = {Efron, Bradley and Tibshirani, Robert},
doi = {10.1080/01621459.1997.10474007},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Efron, Tibshirani - 1997 - Improvements on Cross-Validation The 632 Bootstrap Method.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {CV bias-correction,Classification,Cross-validation bootstrap,Prediction rule},
mendeley-tags = {CV bias-correction},
number = {438},
pages = {548--560},
title = {{Improvements on cross-validation: The .632+ bootstrap method}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
volume = {92},
year = {1997}
}
@techreport{Yang2006,
abstract = {We address the consistency property of cross validation (CV) for classification. Sufficient conditions are obtained on the data splitting ratio to ensure that the better classifier between two candidates will be favored by CV with probability approaching 1. Interestingly, it turns out that for comparing two general learning methods, the ratio of the training sample size and the evaluation size does not have to approach 0 for consistency in selection, as is required for comparing parametric regression models (Shao (1993)). In fact, the ratio may be allowed to converge to infinity or any positive constant, depending on the situation. In addition, we also discuss confidence intervals and sequential instability in selection for comparing classifiers.},
author = {Yang, Yuhong},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yang - 2006 - Comparing Learning Methods for Classification Multi-Armed Bandit with Covariates View project AggregatingCombining Stat(2).pdf:pdf},
keywords = {and phrases: Classification,comparing learning methods,consistency in selection,cross validation paradox,sequential instability Running Title: Comparing Cl},
title = {{Comparing Learning Methods for Classification Multi-Armed Bandit with Covariates View project Aggregating/Combining Statistical Procedures View project Comparing Learning Methods for Classification}},
url = {https://www.researchgate.net/publication/250847095},
year = {2006}
}
@article{Varma2006,
abstract = {Background: Cross-validation (CV) is an effective method for estimating the prediction error of a classifier. Some recent articles have proposed methods for optimizing classifiers by choosing classifier parameter values that minimize the CV error estimate. We have evaluated the validity of using the CV error estimate of the optimized classifier as an estimate of the true error expected on independent data. Results: We used CV to optimize the classification parameters for two kinds of classifiers; Shrunken Centroids and Support Vector Machines (SVM). Random training datasets were created, with no difference in the distribution of the features between the two classes. Using these "null" datasets, we selected classifier parameter values that minimized the CV error estimate. 10-fold CV was used for Shrunken Centroids while Leave-One-Out-CV (LOOCV) was used for the SVM. Independent test data was created to estimate the true error. With "null" anb "non null" (with differential expression between the classes) data, we also tested a nested CV procedure, where an inner CV loop is used to perform the tuning of the parameters while an outer CV is used to compute an estimate of the error. The CV error estimate for the classifier with the optimal parameters was found to be a substantially biased estimate of the true error that the classifier would incur on independent data. Even though there is no real difference between the two classes for the "null" datasets, the CV error estimate for the Shrunken Centroid with the optimal parameters was less than 30{\%} on 18.5{\%} of simulated training data-sets. For SVM with optimal parameters the estimated error rate was less than 30{\%} on 38{\%} of "null" data-sets. Performance of the optimized classifiers on the independent test set was no better than chance. The nested CV procedure reduces the bias considerably and gives an estimate of the error that is very close to that obtained on the independent testing set for both Shrunken Centroids and SVM classifiers for "null" and "non-null" data distributions. Conclusion: We show that using CV to compute an error estimate for a classifier that has itself been tuned using CV gives a significantly biased estimate of the true error. Proper use of CV for estimating true error of a classifier developed using a well defined algorithm requires that all steps of the algorithm, including classifier parameter tuning, be repeated in each CV loop. A nested CV procedure provides an almost unbiased estimate of the true error. {\textcopyright} 2006 Varma and Simon; licensee BioMed Central Ltd.},
author = {Varma, Sudhir and Simon, Richard},
doi = {10.1186/1471-2105-7-91},
file = {::},
issn = {14712105},
journal = {BMC Bioinformatics},
keywords = {Algorithms,Bioinformatics,Computational Biology/Bioinformatics,Computer Appl. in Life Sciences,Microarrays},
month = {feb},
number = {1},
pages = {1--8},
publisher = {BioMed Central},
title = {{Bias in error estimation when using cross-validation for model selection}},
volume = {7},
year = {2006}
}
@book{Davison1997,
abstract = {This book gives a broad and up-to-date coverage of bootstrap methods, with numerous applied examples, developed in a coherent way with the necessary theoretical basis. Applications include stratified data; finite populations; censored and missing data; linear, nonlinear, and smooth regression models; classification; time series and spatial problems. Special features of the book include: extensive discussion of significance tests and confidence intervals; material on various diagnostic methods; and methods for efficient computation, including improved Monte Carlo simulation. Each chapter includes both practical and theoretical exercises. Included with the book is a disk of purpose-written S-Plus programs for implementing the methods described in the text. Computer algorithms are clearly described, and computer code is included on a 3-inch, 1.4M disk for use with IBM computers and compatible machines. Users must have the S-Plus computer application.},
author = {Davison, A. C. and Hinkley, D. V.},
booktitle = {Bootstrap methods and their application},
doi = {10.1017/cbo9780511802843},
isbn = {9780521573917},
month = {oct},
publisher = {Cambridge University Press},
title = {{Bootstrap methods and their application}},
url = {https://www.cambridge.org/core/product/identifier/9780511802843/type/book},
year = {1997}
}
@article{Anderson2006,
abstract = {Produced and posted by David Anderson and Kenneth Burnham. This site will be updated occasionally. The site is a commentary; we have not spent a great deal of time and effort to refine the wording or be comprehensive in any respect. It is informal and we hope people will benefit from our quick thoughts on various matters.},
author = {Anderson, David R and Burnham, Kenneth P},
file = {:C$\backslash$:/Users/layates/Dropbox/DEEP/Model Selection/AIC Myths and Misunderstandings.pdf:pdf},
journal = {Handout},
number = {not 2},
pages = {8},
pmid = {1847},
title = {{AIC Myths and Misunderstandings}},
url = {papers2://publication/uuid/874BB08E-52A9-4099-BB04-70B3250B068E},
year = {2006}
}
@article{Leeb2015,
abstract = {We compare several confidence intervals after model selection in the setting recently studied by Berk et al. [Ann. Statist. 41 (2013) 802-837], where the goal is to cover not the true parameter but a certain nonstandard quantity of interest that depends on the selected model. In particular, we compare the PoSI-intervals that are proposed in that reference with the "naive" confidence interval, which is constructed as if the selected model were correct and fixed a priori (thus ignoring the presence of model selection). Overall, we find that the actual coverage probabilities of all these intervals deviate only moderately from the desired nominal coverage probability. This finding is in stark contrast to several papers in the existing literature, where the goal is to cover the true parameter.},
author = {Leeb, Hannes and P{\"{o}}tscher, Benedikt M. and Ewald, Karl},
doi = {10.1214/14-STS507},
issn = {08834237},
journal = {Statistical Science},
keywords = {AIC,BIC,Confidence intervals,Lasso,Model selection,Nonstandard coverage target},
number = {2},
pages = {216--227},
publisher = {Institute of Mathematical Statistics},
title = {{On various confidence intervals post-model-selection}},
volume = {30},
year = {2015}
}
@article{Arlot2016,
abstract = {This paper studies V-fold cross-validation for model selection in least-squares density estimation. The goal is to provide theoretical grounds for choosing V in order to minimize the least-squares loss of the selected estimator. We first prove a non-asymptotic oracle inequality for V-fold cross-validation and its bias-corrected version (V-fold penalization). In particular, this result implies that V-fold penalization is asymptotically optimal in the nonparametric case. Then, we compute the variance of V-fold cross-validation and related criteria, as well as the variance of key quantities for model selection performance. We show that these variances depend on V like 1+4/(V-1), at least in some particular cases, suggesting that the performance increases much from V=2 to V=5 or 10, and then is almost constant. Overall, this can explain the common advice to take V=5---at least in our setting and when the computational power is limited---, as supported by some simulation experiments. An oracle inequality and exact formulas for the variance are also proved for Monte-Carlo cross-validation, also known as repeated cross-validation, where the parameter V is replaced by the number B of random splits of the data.},
author = {Arlot, Sylvain and Lerasle, Matthieu},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arlot, Lerasle - 2016 - Choice of V for V-Fold Cross-Validation in Least-Squares Density Estimation.pdf:pdf},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {Density estimation,Leave-one-out,Leave-p- out,Model selection,Monte-Carlo cross-validation,Penalization,Resampling penalties,V-fold cross-validation},
pages = {1--50},
title = {{Choice of V for V-fold cross-validation in least-squares density estimation}},
url = {http://www.jmlr.org/papers/volume17/14-296/14-296.pdf},
volume = {17},
year = {2016}
}



@article{Efron2014,
author = {Bradley Efron},
title = {Estimation and Accuracy After Model Selection},
journal = {Journal of the American Statistical Association},
volume = {109},
number = {507},
pages = {991-1007},
year  = {2014},
publisher = {Taylor & Francis},
doi = {10.1080/01621459.2013.823775},
    note ={PMID: 25346558},
URL = {https://doi.org/10.1080/01621459.2013.823775},
eprint = {https://doi.org/10.1080/01621459.2013.823775}
}


@article{Kohavi1995,
abstract = {We review accuracy estimation methods and compare the two most common methods: cross-validation and bootstrap. Recent experimental results on artiicial data and theoretical results in restricted settings have shown that for selecting a good classiier from a set of classi-ers (model selection), tenfold cross-validation may be better than the more expensive l e a ve-one-out cross-validation. We report on a large-scale experiment|over half a million runs of C4.5 and a Naive-Bayes algorithm|to estimate the eeects of diierent parameters on these algorithms on real-world datasets. For cross-validation, we v ary the number of folds and whether the folds are stratiied or nott for boot-strap, we v ary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is tenfold stratiied cross validation, even if computation power allows using more folds.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Kohavi, Ron},
doi = {10.1067/mod.2000.109031},
eprint = {arXiv:1011.1669v3},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kohavi - 1995 - A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.pdf:pdf},
isbn = {1-55860-363-8},
issn = {10450823},
keywords = {CV bias-correction},
mendeley-tags = {CV bias-correction},
pages = {7},
pmid = {11029742},
title = {{A study of cross-validation and bootstrap for accuracy estimation and model selection”. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence}},
url = {http://robotics.stanford.edu/{~}ronnyk},
volume = {5},
year = {1995}
}
@article{Nakagawa2013,
abstract = {The use of both linear and generalized linear mixed-effects models (LMMs and GLMMs) has become popular not only in social and medical sciences, but also in biological sciences, especially in the field of ecology and evolution. Information criteria, such as Akaike Information Criterion (AIC), are usually presented as model comparison tools for mixed-effects models. The presentation of 'variance explained' (R2) as a relevant summarizing statistic of mixed-effects models, however, is rare, even though R2 is routinely reported for linear models (LMs) and also generalized linear models (GLMs). R2 has the extremely useful property of providing an absolute value for the goodness-of-fit of a model, which cannot be given by the information criteria. As a summary statistic that describes the amount of variance explained, R2 can also be a quantity of biological interest. One reason for the under-appreciation of R2 for mixed-effects models lies in the fact that R2 can be defined in a number of ways. Furthermore, most definitions of R2 for mixed-effects have theoretical problems (e.g. decreased or negative R2 values in larger models) and/or their use is hindered by practical difficulties (e.g. implementation). Here, we make a case for the importance of reporting R2 for mixed-effects models. We first provide the common definitions of R2 for LMs and GLMs and discuss the key problems associated with calculating R2 for mixed-effects models. We then recommend a general and simple method for calculating two types of R2 (marginal and conditional R2) for both LMMs and GLMMs, which are less susceptible to common problems. This method is illustrated by examples and can be widely employed by researchers in any fields of research, regardless of software packages used for fitting mixed-effects models. The proposed method has the potential to facilitate the presentation of R2 for a wide range of circumstances. {\textcopyright} 2012 The Authors. Methods in Ecology and Evolution {\textcopyright} 2012 British Ecological Society.},
author = {Nakagawa, Shinichi and Schielzeth, Holger},
doi = {10.1111/j.2041-210x.2012.00261.x},
editor = {O'Hara, Robert B.},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Nakagawa2013.pdf:pdf},
issn = {2041210X},
journal = {Methods in Ecology and Evolution},
keywords = {Coefficient of determination,Goodness-of-fit,Heritability,Information criteria,Intra-class correlation,Linear models,Model fit,Repeatability,Variance explained},
month = {feb},
number = {2},
pages = {133--142},
title = {{A general and simple method for obtaining R2 from generalized linear mixed-effects models}},
url = {http://doi.wiley.com/10.1111/j.2041-210x.2012.00261.x},
volume = {4},
year = {2013}
}
@techreport{Yanagihara2013,
abstract = {In this paper, we define a class of cross-validatory model selection criteria as an estimator of the predictive risk function based on a discrepancy between a candidate model and the true model. For a vector of unknown parameters, n estimators are required for the definition of the class, where n is the sample size. The ith estimator ði ¼ 1;. .. ; nÞ is obtained by minimizing a weighted discrepancy function in which the ith observation has a weight of 1 {\`{A}} l and others have weight of 1. Cross-validatory model selection criteria in the class are specified by the individual l. The sample discrepancy function and the ordinary cross-validation (CV) criterion are special cases of the class. One may choose l to minimize the biases. The optimal l makes the bias-corrected CV (CCV) criterion a second-order unbiased estimator for the risk function, while the ordinary CV criterion is a first-order unbiased estimator of the risk function.},
author = {Yanagihara, Hirokazu and Yuan, Ke-Hai and Fujisawa, Hironori and Hayashi, Kentaro},
booktitle = {Hiroshima Math. J},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yanagihara et al. - 2013 - A class of cross-validatory model selection criteria.pdf:pdf},
number = {Cv},
pages = {1--30},
title = {{A Class of Model Selection Criteria Based on Cross-Validation Method}},
url = {https://projecteuclid.org/download/pdf{\_}1/euclid.hmj/1372180510},
volume = {43},
year = {2007}
}
@article{Berk2013,
abstract = {It is common practice in statistical data analysis to perform data-driven variable selection and derive statistical inference from the resulting model. Such inference enjoys none of the guarantees that classical statistical theory provides for tests and confidence intervals when the model has been chosen a priori. We propose to produce valid "post-selection inference" by reducing the problem to one of simultaneous inference and hence suitably widening conventional confidence and retention intervals. Simultaneity is required for all linear functions that arise as coefficient estimates in all submodels. By purchasing "simultaneity insurance" for all possible submodels, the resulting post-selection inference is rendered universally valid under all possible model selection procedures. This inference is therefore generally conservative for particular selection procedures, but it is always less conservative than full Scheff{\'{e}} protection. Importantly it does not depend on the truth of the selected submodel, and hence it produces valid inference even in wrong models. We describe the structure of the simultaneous inference problem and give some asymptotic results. {\textcopyright} Institute of Mathematical Statistics, 2013.},
author = {Berk, Richard and Brown, Lawrence and Buja, Andreas and Zhang, Kai and Zhao, Linda},
doi = {10.1214/12-AOS1077},
file = {::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Family-wise error,High-dimensional inference,Linear regression,Model selection,Multiple comparison,Sphere packing},
month = {apr},
number = {2},
pages = {802--837},
title = {{Valid post-selection inference}},
volume = {41},
year = {2013}
}

 @manual{bestglm,
    title = {bestglm: Best Subset GLM and Regression Utilities},
    author = {A.I. McLeod and Changjiang Xu and Yuanhao Lai},
    year = {2020},
    note = {R package version 0.37.3},
    url = {https://CRAN.R-project.org/package=bestglm},
  }

  @Misc{loo,
    title = {loo: Efficient leave-one-out cross-validation and WAIC for Bayesian models},
    author = {Aki Vehtari and Jonah Gabry and Mans Magnusson and Yuling Yao and Andrew Gelman},
    year = {2019},
    note = {R package version 2.2.0},
    url = {https://mc-stan.org/loo},
  }


  @Misc{rstanarm,
    title = {rstanarm: {Bayesian} applied regression modeling via {Stan}.},
    author = {Ben Goodrich and Jonah Gabry and Imad Ali and Sam Brilleman},
    note = {R package version 2.19.3},
    year = {2020},
    url = {https://mc-stan.org/rstanarm},
  }


@manual{caret,
    title = {caret: Classification and Regression Training},
    author = {Max Kuhn},
    year = {2020},
    note = {R package version 6.0-86},
    url = {https://CRAN.R-project.org/package=caret},
  }
  
  @manual{Rcore,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2020},
    url = {https://www.R-project.org/},
  }  
  
@techreport{Vehtari2004,
abstract = {We consider model selection as a decision problem from a predictive perspective. The optimal Bayesian way of handling model uncertainty is to integrate over model space. Model selection can then be seen as point estimation in the model space. We propose a model selection method based on Kullback-Leibler divergence from the predictive distribution of the full model to the predictive distributions of the submodels. The loss of predictive explanatory power is defined as the expectation of this predictive discrepancy. The goal is to find the simplest submodel which has a similar predictive distribution as the full model, that is, the simplest submodel whose loss of explanatory power is acceptable. To compute the expected predictive discrepancy between complex models, for which analytical solutions do not exist, we propose to use predictive distributions obtained via k-fold cross-validation. We compare the performance of the method to posterior probabilities (Bayes factors), deviance information criteria (DIC) and direct maximization of the expected utility via cross-validation.},
author = {Vehtari, Aki and Lampinen, Jouko},
institution = {Helsinki University of Technology},
file = {::},
isbn = {951-22-7190-7},
issn = {1455-0474},
keywords = {Bayesian model choice,DIC,covariate selection,cross-validation,decision theory,expected utility},
title = {{Model Selection via Predictive Explanatory Power}},
url = {https://www.researchgate.net/publication/228929524},
year = {2004}
}


@article{Gneiting2007,
abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distribution F if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage. {\textcopyright} 2007 American Statistical Association.},
author = {Gneiting, Tilmann and Raftery, Adrian E},
doi = {10.1198/016214506000001437},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gneiting, Raftery - 2007 - Strictly Proper Scoring Rules, Prediction, and Estimation.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {Bayes factor,Bregman divergence,Brier score,Coherent,Continuous ranked probability score,Cross-validation,Entropy,Kernel score,Loss function,Minimum contrast estimation,Negative definite function},
number = {477},
pages = {359--378},
title = {{Strictly proper scoring rules, prediction, and estimation}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=uasa20},
volume = {102},
year = {2007}
}
@misc{Bolker2009,
abstract = {How should ecologists and evolutionary biologists analyze nonnormal data that involve random effects? Nonnormal data such as counts or proportions often defy classical statistical procedures. Generalized linear mixed models (GLMMs) provide a more flexible approach for analyzing nonnormal data when random effects are present. The explosion of research on GLMMs in the last decade has generated considerable uncertainty for practitioners in ecology and evolution. Despite the availability of accurate techniques for estimating GLMM parameters in simple cases, complex GLMMs are challenging to fit and statistical inference such as hypothesis testing remains difficult. We review the use (and misuse) of GLMMs in ecology and evolution, discuss estimation and inference and summarize 'best-practice' data analysis procedures for scientists facing this challenge. {\textcopyright} 2008 Elsevier Ltd. All rights reserved.},
author = {Bolker, Benjamin M. and Brooks, Mollie E. and Clark, Connie J. and Geange, Shane W. and Poulsen, John R. and Stevens, M. Henry H. and White, Jada Simone S.},
booktitle = {Trends in Ecology and Evolution},
doi = {10.1016/j.tree.2008.10.008},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bolker et al. - 2009 - Generalized linear mixed models a practical guide for ecology and evolution.pdf:pdf},
issn = {01695347},
month = {mar},
number = {3},
pages = {127--135},
pmid = {19185386},
title = {{Generalized linear mixed models: a practical guide for ecology and evolution}},
volume = {24},
year = {2009}
}
@article{Vaida2005,
abstract = {This paper focuses on the Akaike information criterion, AIC, for linear mixed-effects models in the analysis of clustered data. We make the distinction between questions regarding the population and questions regarding the particular clusters in the data. We show that the AIC in current use is not appropriate for the focus on clusters, and we propose instead the conditional Akaike information and its corresponding criterion, the conditional AIC, cAIC. The penalty term in cAIC is related to the effective degrees of freedom $\rho$ for a linear mixed model proposed by Hodges {\&} Sargent (2001); $\rho$ reflects an intermediate level of complexity between a fixed-effects model with no cluster effect and a corresponding model with fixed cluster effects. The cAIC is defined for both maximum likelihood and residual maximum likelihood estimation. A pharmacokinetics data application is used to illuminate the distinction between the two inference settings, and to illustrate the use of the conditional AIC in model selection. {\textcopyright} 2005 Biometrika Trust.},
author = {Vaida, Florin and Blanchard, Suzette},
doi = {10.1093/biomet/92.2.351},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd/Mendeley Desktop/Downloaded/Vaida2005.pdf:pdf},
issn = {00063444},
journal = {Biometrika},
keywords = {AIC,Akaike information,Effective degrees of freedom,Linear mixed model},
month = {jun},
number = {2},
pages = {351--370},
title = {{Conditional Akaike information for mixed-effects models}},
url = {http://academic.oup.com/biomet/article/92/2/351/233128/Conditional-Akaike-information-for-mixedeffects},
volume = {92},
year = {2005}
}
@article{Piironen2015,
abstract = {This document is additional material to our previous study comparing several strategies for variable subset selection. Our recommended approach was to fit the full model with all the candidate variables and best possible prior information, and perform the variable selection using the projection predictive framework. Here we give an example of performing such an analysis, using Stan for fitting the model, and R for the variable selection.},
archivePrefix = {arXiv},
arxivId = {1508.02502},
author = {Piironen, Juho and Vehtari, Aki},
eprint = {1508.02502},
file = {::},
month = {aug},
title = {{Projection predictive variable selection using Stan+R}},
url = {http://arxiv.org/abs/1508.02502},
year = {2015}
}
@article{Tibshirani2009,
abstract = {Tuning parameters in supervised learning problems are often estimated by cross-validation. The minimum value of the cross-validation error can be biased downward as an estimate of the test error at that same value of the tuning parameter. We propose a simple method for the estimation of this bias that uses information from the cross-validation process. As a result, it requires essentially no additional computation. We apply our bias estimate to a number of popular classifiers in various settings, and examine its performance.},
author = {Tibshirani, Ryan J. Robert and Tibshirani, Ryan J. Robert},
doi = {10.1214/08-AOAS224},
file = {::},
issn = {19326157},
journal = {Annals of Applied Statistics},
keywords = {CV bias-correction,Cross-validation,Optimism estimation,Prediction error estimation},
mendeley-tags = {CV bias-correction},
number = {2},
pages = {822--829},
title = {{A bias correction for the minimum error rate in cross-validation}},
url = {https://www.jstor.org/stable/pdf/30244266.pdf?refreqid=excelsior{\%}3A9f665d85e42817493b52b4bda24caf0e},
volume = {3},
year = {2009}
}
@article{Bousquet2002,
abstract = {We define notions of stability for learning algorithms and show how to use these notions to derive generalization error bounds based on the empirical error and the leave-one-out error. The methods we use can be applied in the regression framework as well as in the classification one when the classifier is obtained by thresholding a real-valued function. We study the stability properties of large classes of learning algorithms such as regularization based algorithms. In particular we focus on Hubert space regularization and Kullback-Leibler regularization. We demonstrate how to apply the results to SVM for regression and classification.},
author = {Bousquet, Olivier and Elisseeff, Andr{\'{e}}},
doi = {10.1162/153244302760200704},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bousquet, Elisseeff - 2002 - Stability and Generalization.pdf:pdf},
issn = {15324435},
journal = {Journal of Machine Learning Research},
number = {3},
pages = {499--526},
title = {{Stability and Generalization}},
url = {http://sensitivity-analysis.jrc.cec.eu.int/},
volume = {2},
year = {2002}
}
@article{Bachoc2019,
abstract = {We consider inference post-model-selection in linear regression. In this setting, Berk et al. [Ann. Statist. 41 (2013a) 802–837] recently introduced a class of confidence sets, the so-called PoSI intervals, that cover a certain nonstandard quantity of interest with a user-specified minimal coverage probability, irrespective of the model selection procedure that is being used. In this paper, we generalize the PoSI intervals to confidence intervals for post-model-selection predictors.},
archivePrefix = {arXiv},
arxivId = {1412.4605},
author = {Bachoc, Fran{\c{c}}ois and Leeb, Hannes and P{\"{o}}tscher, Benedikt M},
doi = {10.1214/18-AOS1721},
eprint = {1412.4605},
file = {::},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Confidence intervals,Inference post-model-selection,Linear regression,Nonstandard targets,Optimal post-model-selection predictors},
number = {3},
pages = {1475--1504},
title = {{Valid confidence intervals for post-model-selection predictors}},
url = {https://doi.org/10.1214/18-AOS1721},
volume = {47},
year = {2019}
}
@misc{Burnham2004a,
abstract = {The model selection literature has been generally poor at reflecting the deep foundations of the Akaike information criterion (AIC) and at making appropriate comparisons to the Bayesian information criterion (BIC). There is a clear philosophy, a sound criterion based in information theory, and a rigorous statistical foundation for AIC. AIC can be justified as Bayesian using a "savvy" prior on models that is a function of sample size and the number of model parameters. Furthermore, BIC can be derived as a non-Bayesian result. Therefore, arguments about using AIC versus BIC for model selection cannot be from a Bayes versus frequentist perspective. The philosophical context of what is assumed about reality, approximating models, and the intent of model-based inference should determine whether AIC or BIC is used. Various facets of such multimodel inference are presented here, particularly methods of model averaging.},
author = {Burnham, Kenneth P. and Anderson, David R.},
booktitle = {Sociological Methods and Research},
doi = {10.1177/0049124104268644},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Burnham, Anderson - 2004 - Multimodel inference Understanding AIC and BIC in model selection.pdf:pdf},
issn = {00491241},
keywords = {AIC,BIC,Model averaging,Model selection,Multimodel inference},
month = {nov},
number = {2},
pages = {261--304},
title = {{Multimodel inference: Understanding AIC and BIC in model selection}},
url = {http://journals.sagepub.com/doi/10.1177/0049124104268644},
volume = {33},
year = {2004}
}
@article{Kabaila2019,
abstract = {Bootstrap smoothed (bagged) estimators have been proposed as an improvement on estimators found after preliminary data-based model selection. Efron derived a widely applicable formula for a delta method approximation to the standard deviation of the bootstrap smoothed estimator. He also considered a confidence interval centred on the bootstrap smoothed estimator, with width proportional to the estimate of this standard deviation. Recently, Kabaila and Wijethunga assessed the performance of this confidence interval in the scenario of two nested linear regression models, the full model and the simpler model, for the case of known error variance and preliminary model selection using a hypothesis test. They found that the performance of this confidence interval was not substantially better than the usual confidence interval based on the full model, with the same minimum coverage. We extend this assessment to the case of unknown error variance by deriving a computationally convenient exact formula for the ideal (i.e., in the limit as the number of bootstrap replications diverges to infinity) delta method approximation to the standard deviation of the bootstrap smoothed estimator. Our results show that, unlike the known error variance case, there are circumstances in which this confidence interval has attractive properties.},
author = {Kabaila, Paul and Wijethunga, Christeen},
doi = {10.1002/sta4.233},
issn = {20491573},
journal = {Stat},
keywords = {bootstrap smoothed estimator,confidence interval,coverage probability,expected length,model selection},
month = {jan},
number = {1},
publisher = {Wiley-Blackwell Publishing Ltd},
title = {{On confidence intervals centred on bootstrap smoothed estimators}},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.233},
volume = {8},
year = {2019}
}
@article{Kabaila2016a,
abstract = {We develop an approach to evaluating frequentist model averaging procedures by considering them in a simple situation in which there are two-nested linear regression models over which we average. We introduce a general class of model averaged confidence intervals, obtain exact expressions for the coverage and the scaled expected length of the intervals, and use these to compute these quantities for the model averaged profile likelihood (MPI) and model-averaged tail area confidence intervals proposed by D. Fletcher and D. Turek. We show that the MPI confidence intervals can perform more poorly than the standard confidence interval used after model selection but ignoring the model selection process. The model-averaged tail area confidence intervals perform better than the MPI and postmodel-selection confidence intervals but, for the examples that we consider, offer little over simply using the standard confidence interval for $\theta$ under the full model, with the same nominal coverage.},
author = {Kabaila, Paul and Welsh, A. H. and Abeysekera, Waruni},
doi = {10.1111/sjos.12163},
issn = {14679469},
journal = {Scandinavian Journal of Statistics},
keywords = {Akaike information criterion,Confidence interval,Coverage probability,Expected length,Model selection,Nominal coverage,Profile likelihood,Regression models,Tail area confidence interval},
month = {mar},
number = {1},
pages = {35--48},
publisher = {Blackwell Publishing Ltd},
title = {{Model-Averaged Confidence Intervals}},
url = {http://doi.wiley.com/10.1111/sjos.12163},
volume = {43},
year = {2016}
}



@article{Efron2004,
author = {Bradley Efron},
title = {The Estimation of Prediction Error},
journal = {Journal of the American Statistical Association},
volume = {99},
number = {467},
pages = {619-632},
year  = {2004},
publisher = {Taylor & Francis},
doi = {10.1198/016214504000000692},
URL = {https://doi.org/10.1198/016214504000000692},
eprint = {https://doi.org/10.1198/016214504000000692}
}

\textbf{}
@article{Arlot2009a,
abstract = {We present a new family of model selection algorithms based on the resampling heuristics. It can be used in several frameworks, do not require any knowledge about the unknown law of the data, and may be seen as a generalization of local Rademacher complexities and V-fold cross-validation. In the case example of least-square regression on histograms, we prove oracle inequalities, and that these algorithms are naturally adaptive to both the smoothness of the regression function and the variability of the noise level. Then, interpretating V-fold cross-validation in terms of penalization, we enlighten the question of choosing V. Finally, a simulation study illustrates the strength of resampling penalization algorithms against some classical ones, in particular with heteroscedastic data.},
author = {Arlot, Sylvain},
doi = {10.1214/08-EJS196},
file = {:C$\backslash$:/Users/layates/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Arlot - 2009 - Model selection by resampling penalization(2).pdf:pdf},
issn = {19357524},
journal = {Electronic Journal of Statistics},
keywords = {Adaptivity,CV bias-correction,Exchangeable weighted bootstrap,Heteroscedastic data,Histogram selection,Model selection,Non-parametric regression,Non-parametric statistics,Penalization,Regressogram,Resampling},
mendeley-tags = {CV bias-correction},
pages = {557--624},
title = {{Model selection by resampling penalization}},
url = {https://projecteuclid.org/download/pdfview{\_}1/euclid.ejs/1245415825},
volume = {3},
year = {2009}
}
